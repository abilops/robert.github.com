---
layout: post
title: How to work out when an audio clip was recorded by using the mains hum
tags: [TODO]
published: false
---
Electricity is transmitted around most countries by a large network of cables and wires called a *grid*. As the grid pipes power around, it emits a deep, quiet hum. This "mains hum" seeps into microphones and recordings; a pain for sound engineers, but surprisingly useful for intelligence analysts. An analyst - or other interested busybody - can use fluctuations in the mains hum on a recording to deduce the time at which the recording was taken, to within a few seconds. This accidental technique has been used to verify a crucial piece of evidence [in the trial of a group of arms dealers](https://www.bbc.co.uk/news/science-environment-20629671).

This post is the first accessible, non-academic explanation of the details of *Electrical Network Frequency (ENF) matching*, and the [companion repo](https://github.com/robert/enf-matching) is the first open source proof-of-concept. Let's see how EMF matching works.

### Where does the mains hum come from?

Power is transmitted through the electrical grid as alternating current (AC). This means that the current that flows through its cables is constantly changing direction. In most countries - including the UK, where I live - the direction switches 50 times a second, or 50Hz. In other countries, like the US and Canada, it switches 60 times a second, or 60Hz. These grids' Electrical Network Frequencies (ENF).  For the rest of this post we'll talk from the point of view of the UK and other 50Hz countries.

<img src="/images/enf/basic-ac.png" />

This electrical back-and-forth causes some of the components that the current passes through to vibrate, very slightly, at the same frequency as the AC oscillation. This vibration causes a barely audible sound, also at 50Hz.  This sound is the mains hum.

The UK's National Grid targets an AC frequency of 50Hz. But in practice the frequency varies slightly and randomly around this target, as the power demands on the grid fluctuate (49.8Hz-50.2Hz is a normal range). The grid is a single, connected system, so when its frequency wanders, it wanders in synchrony across the whole country.

[PIC?]

When the mains hum is picked up on a recording, its frequency fluctuations are picked up too. If we isolate and analyse the hum in a clip, we can measure these tiny variations in frequency. Because the variations are random, patterns don't (or at least rarely) repeat. This means that the way in which the ENF varies during a recording can be used as a fingerprint that uniquely (ish) identifies the time at which the recording was made. We can timestamp a clip by comparing its ENF series to a database of past ENF values, and find the time at which the recording's ENF most closely matches history. Second-by-second databases of past ENFs are widely available for many grids, sometimes published by grid operators themselves, and sometimes by individuals or organisations.

<img src="/images/enf/basic-matching-2.png" />

That's the idea; let's look at the details.

### Overview

We're going to use ENF matching to answer the question "here's a recording, when was it was (probably) taken?" I say "probably" because all that ENF matching can give us is a statistical best guess, not a guarantee. The mains hum isn't always clearly present on recordings, and even if it is, our target recording's ENF can match with a wrong section of the reference ENF database that just happens to look very similar, by pure misfortune. 

Still, even though all ENF matching gives us is a guess, it's usually a good one. The longer the recording, the more accurate the technique; 10 minutes is often given as a lower bound in the academic literature.

To make our guess, we'll need to:

1. Extract the target recording's ENF values over time
2. Find a database of reference ENF values, taken directly from the electrical grid
3. Find the section of the reference ENF series that best matches the target. This section is our best guess for when the target recording was taken

### 1. Extract the ENF from the target recording

In this first step we extract the ENF and its variations from our target recording. At the end we'll have a list of values of the ENF over time, one for each second of the recording.

```
[50.225, 50.228, 50.330, 50.227, ...etc...]
```

To extract the ENF from our target recording, we'll need to:

1. Downsample the recording to reduce its size (optional, but speeds up later steps)
2. Remove all frequencies outside of a narrow band around the nominal ENF using a bandpass filter
3. Estimate the ENF for each second by running a Short Time Fourier Transform (STFT) on the filtered recording and finding peak frequencies

Let's start with downsampling.

#### 1.1 Downsample the recording

Recordings from many audio devices are of a much higher quality than we need in order to extract the ENF. Our algorithm works fine on these high quality files, but takes a long time. To save computation time in later steps, we can reduce the quality (or *downsample*, or *decimate*) the recording before processing it.

To do this we need to reduce the *sample rate* of the file. The sample rate of a digital audio file is the number of measurements of the audio wave's amplitude, per second, contained in the file. The sample rate of an audio file is somewhat analogous to the number of pixels in an image. The details of sample rates aren't important to us, but what is important is the fact that in order for a file to accurately represent a sound of a particular frequency, its sample rate needs to be at least twice the sound's frequency. The ENF is around 50Hz, so our target file's sample rate needs to be at least 100Hz in order to accurately capture the mains hum (a slightly higher sample rate - 300Hz - is suggested in [Huijbregtse, Geradts, (2009)](http://www.forensic.to/ENF%20processed.pdf), although without justification). Beyond this, higher sample rates give no discernible improvement in audio quality for low-end frequencies like the ENF.

However, many recording devices (such as my iPhone) record audio using a much more generous sample rate of 44,100Hz. This is because the highest frequency that humans can hear is about 20,000Hz, and so devices need a sample rate of at least 40,000Hz in order to capture the full range of human hearing. They add an extra few thousand samples per second beyond this for reasons that we don't need to worry about. Either way, this sample rate is much higher than the 300Hz or so that we need in order to capture the ENF. This means that we can safely reduce the sample rate of our target recording, using an established downsampling algorithm, without any effect on our results. This saves time later because in future steps we're going to need to iterate over every sample in our target clip. A clip with a sample rate of 300Hz contains less than 1% of the number of samples as that same clip at 44,100Hz. Fewer samples means less iteration time, and a faster algorithm.

Once we've slimmed down our target recording, we can start to isolate the mains hum.

### 2. Bandpass filter

The ENF varies around its nominal value of 50Hz, but not by much. In the UK, I've only seen values between about 49.8Hz and 50.2Hz, although fluctuations in other grids may be larger or smaller. Frequencies outside this plausible range are useless to us, so we filter them out in order to enhance and simplify the rest of our analysis.

<img src="/images/enf/filtered-spectrum.png" />

To remove all but a specified band of frequencies from a clip, we pass the clip through a function called a *bandpass filter*. Different flavours of bandpass filter cut and preserve frequencies in slightly different ways, and I chose a type called a "Butterworth Filter" because it's designed to disturb the frequencies within the band (the part of the sound that we care about) as little as possible. Bandpass filters aren't perfect. They don't completely remove frequencies outside their band, and they may slightly alter frequencies within it. We can accept some blemishes, but the more precise our filter, the more accurate the rest of our algorithm. I chose a filter *order* of 10, which is quite high, because the higher the order, the sharper the cutoff at the boundaries.

We've created an audio clip that contains only frequencies within a narrow range around the nominal ENF. Now we're ready to extract the true ENF as it varies throughout our recording.

### 3. Estimate the ENF using STFT

In order to extract the ENF and its variations from our filtered clip, we need to determine the dominant frequency at each second. This will be our estimate for the ENF at that second. To do this, we need to:

1. Calculate the amplitude of the different frequencies in the signal each second (a *frequency spectrum*)
2. Find the highest amplitude frequency in each spectrum

#### 1. Calculate the amplitude of the different frequencies in the signal each second

An equaliser (EQ) display on a hi-fi or mixing desk shows the amplitude of different frequencies on a track. It shows how much bass, mid, and high-end is in a sound.

<img src="/images/enf/hifi-eq.png" />

Frequency spectra like those on an EQ are calculated using a family of mathematical processes called *Fourier Transforms*. To calculate how our clip's spectrum varies over time, we'll use a variant called a *Short-Time Fourier Transform* (STFT).

A vanilla Fourier Transform tells us how much of each frequency a wave contains. If we ran a Fourier Transform on a clip of someone playing a C major chord (C-E-G), we would expect it to contain 3 peaks at the frequencies of C, E, and G, ignoring *harmonics* and other colourations of the sound.

<img src="/images/enf/c-major.png" />

A vanilla Fourier Transform performs this decomposition once, for the entirety of a wave. However, we don't want a single spectrum representing our whole clip. Instead, we want to know how our clip's frequency content changes over time. An STFT achieves this using multiple, smaller Fourier Transforms. It divides the signal into small time buckets, and performs a separate transform on the portion of the signal in each bucket. It outputs one spectra for each bucket, instead of a single spectra for the whole wave.

<img src="/images/enf/stft-summary.png" />

To smooth its results, an STFT doesn't process each bucket in isolation. Instead, it uses a sliding window, performing the Fourier Transform on a window that covers both the target bucket and its adjacent buckets. It then shifts the window one step to the side in order to analyse the next target bucket. The caller of the STFT can specify how wide the window should be. 

<img src="/images/enf/stft-sliding-window.png" />

In ENF analysis we need to obtain one frequency spectrum for each second, so we set our bucket width to one second. For window size, [Hua et al 2017](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7807225) asserts that 8 or 16 buckets gives the best results.

Note that each value in a Fourier Transform has two components (represented as a *complex number*) to indicate the *phase* of the frequency at that point. We don't need this information (and we don't need to worry about what it means), so we turn the two components into a single value by taking their combined amplitude.

An STFT gives us an estimate of the frequency spectrum for each second. We expect it to only find frequencies within, or at least very close to, the range of our bandpass filter (say, 49.8Hz - 50.2Hz), because our filter should have removed all other frequencies.

<img src="/images/enf/ft-output.png" />

We now use each second's frequency spectrum to give the single frequency that we will use as our best guess for the ENF at that second in the recording. The simplest estimate would be the frequency in the spectrum with the highest amplitude. However, this would be a small error. We've used a *discrete* STFT, which means that its output isn't a continuous function that gives us a value for every possible frequency, to arbitrary precision. Instead, it gives us an approximation of a continuous Fourier Transform, represented by a finite number of sampled values. For example, a discrete STFT might return values for the content at 49.800Hz, 49.825Hz, 49.850Hz, and so on, but not for any frequencies between these samples.

<img src="/images/enf/ft-max-simple.png" />

It's unlikely that the true highest amplitude frequency falls exactly on the highest amplitude sample, even if this is the highest value that we directly observe.

<img src="/images/enf/ft-true-max.png" />

To improve our estimate of the true peak, we use a technique called *quadratic interpolation* to infer what might be happening between samples. We use the values either side of the maximum sample to infer the shape that the true, continuous curve takes around this maximum.

The quadratic interpolation formula is:

```
peak = (
    0.5 *
    (amp(max_f - 1) - amp(max_f + 1)) /
    (amp(max_f - 1) - 2*amp(max_f) + amp(max_f + 1))
)
```

where:

* `max_f` is the sampled frequency with the highest amplitude
* `amp(F)` is the sampled amplitude at frequency `F`

See eg. [these lecture notes](https://ccrma.stanford.edu/~jos/sasp/Quadratic_Interpolation_Spectral_Peaks.html) for further details.

Once we've estimated each second's maximum amplitude frequency, we will have produced a second-by-second estimate of the ENF from our recording, as desired. This estimate can be further improved, for example by combining information from the spectra at multiple harmonics (eg. 50Hz, 100Hz, 150Hz) ([Hua et al, 2021](https://arxiv.org/abs/2011.03414)), but these improvements are complex and relatively marginal. If implemented they will tweak the values in the ENF series that this step outputs, but won't affect the rest of the algorithm.

With our target recording turned into an ENF series, we now need a reference to compare it against.

### 2. Find a database of reference ENF values from the electrical grid

We need a reference series of true ENF values, taken directly from the grid, every second. These are sometimes published by the grid operator itself (for example, [the UK's National Grid](https://data.nationalgrideso.com/system/system-frequency-data)), and sometime by other organisations or individuals (for example, [power-grid-frequency.org](https://power-grid-frequency.org/)). In the worst case, it doesn't seem too hard to record the ENF yourself, although I've never tried, and this doesn't help if you're trying to timestamp a historical clip.

The matching process will be faster and more accurate if we can establish a time range in which the target recording was taken - for example on a specific day or week - and restrict the reference series to only that period. This limits the amount of reference data that we have to process, and reduces the probability of a spurious false positive.

We download the ENF database and filter it to as tight a date range as we can. We're now ready for the final step: matching the two series together.

### 3. Match the target ENF series to the reference series

We need to find the section of the reference ENF series that most closely matches the target. This will give us our estimate for the time at which the target recording was made.

We start by taking a segment from the start of the reference series, of the same length as the whole target series. We compute and record the similarity of the two series (more on which shortly). We then take another segment from the reference, starting one second later, and compute and record the similarity again. We repeat this across the whole reference series.

<img src="/images/enf/enf-compare.png" />

Once we've shifted and compared the target across all points in the reference, we decide which point was the best match. This is then our best guess for the time at which the recording was made.

In order to objectively compare the similarity of the target and reference ENFs at different points, we need to be able to numerically measure the similarity between two series. The gauge used in the papers that I've read is the *Product Moment Correlation Co-Efficient* (PMCC). The PMCC is a measure of the linear correlation between two sets of paired data. Do the sets tend to increase and decrease together, inversely, or is there no pattern?

<img src="/images/enf/pmcc-basics.png" />

We can pair up the values of the target and reference ENF series from the same offsets, and calculate the PMCC between this paired data. If the target and reference values tend to move up and down in tandem, then this suggests that the target may have been recorded at the same time as the reference. The two series will have a high correlation co-efficient.

Once we've found the alignment with the highest PMCC, we're done.

Let's finish with a few finer points.

### Finer points

#### What if there's no mains hum?

Not all recordings contain a mains hum at 50Hz, perhaps because the recording wasn't taken near any cables or devices emitting the hum. If our target recording doesn't contain any mains hum then we need to abandon our prediction. The ENF matching algorithm will still produce a guess, even when no real hum is present, and we need to avoid making spurious predictions and assertions based on imaginary data. [Hua et al 2021](https://ieeexplore.ieee.org/document/9143185) proposes an algorithm to systematically determine whether mains hum is present in a recording, but for an individual case it's straightforward to manually inspect the output of the ENF extraction step to see whether a pronounced hum is present. The peak in each second's frequency spectrum should be relatively pronounced, and the ENF should vary slowly, rarely much more than 0.001Hz in a second. Shallow peaks or an erratic ENF suggest that there's no hum to find.

#### Using harmonics

Even if a recording is taken in an area where a mains hum is present, the hum may not make its way onto the clip. As Guang Hua, author of many important papers on ENF matching, pointed out to me in an email exchange, some devices deliberately filter out frequencies below 100Hz for the sake of efficiency.

However, we may still be able to detect the hum at higher frequencies. The hum isn't a single, pure tone. It has a *fundamental frequency* at 50Hz, but it also has *harmonic frequencies* at multiples of 50Hz (so 100Hz, 150Hz, 200Hz, etc). This means that if we want to perform ENF matching on a recording containing little-to-no hum at 50Hz, we may instead be able to perform the same analysis on the first harmonic, at 100Hz. If we can extract this harmonic and its variation over time, then we can divide each value in the series by two to recover the fundamental ENF.

This is much better than nothing, but these higher frequencies are more vulnerable to noise from other elements on the recording. For example, [a typical adult man's voice contains frequencies between 85-155Hz](https://en.wikipedia.org/wiki/Voice_frequency), which overlaps with the first (100Hz) and second (150Hz) ENF harmonics. The more overlaps with the hum frequency, the harder it is to distinguish the hum from other "noise" on the recording. By contrast, the lower, fundamental frequency range around 50Hz is comparatively unused, and so is likely to be less affected by other elements on the clip. To attempt to get the best of all worlds, [Hua et al, 2021](https://arxiv.org/abs/2011.03414) suggests a way to combine harmonics and the fundamental frequency to produce more accurate ENF estimates. I haven't looked into this approach in detail yet.

#### Are sub-second offsets a problem?

Recordings will almost certainly never start on an exact second. They'll almost certainly start part way through a second, and so the second-by-second ENF values from the target recording will be taken at a constant, sub-second offset from the reference.

<img src="/images/enf/sub-second-offset.png" />

Fortunately, our algorithm is not significantly affected by this small shift. The ENF varies slowly, and so its value at `t` will almost always be very similar to its value at `t+1`. That means that if a target and reference ENF series are highly correlated, they'll still be highly correlated when shifted by half a second. Indeed, when the algorithm gives its best guess as time `T`, it often gives its next best guesses as `T+1` and `T-1`.

### Conclusion

ENF matching is a strange, accidental technique that shows how easy it is to leak information via the msot esoteric of channels. Researchers continue to further amplify the power of ENF matching and other multimedia forensic tools. For example, some are attempting to use ENF matching principles to timestamp silent video footage by inferring variations in ENF using the imperceptible flicker of ceiling lights instead of audio hum.

I've written a proof-of-concept implementation of ENF matching and [published the code](https://github.com/robert/enf-matching). If you'd like to build your own version, I've also published X sample recordings and their true timestamps that you can use to test your code. Let me know how you get on or if you have any questions.