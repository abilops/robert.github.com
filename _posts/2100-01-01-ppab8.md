---
layout: post
title: "PFAB #8: Input validation and the tradeoffs between convenience and surprise"
published: false
---


Most input validations are trivial, and simply check whether an input makes any sense at all. Clearly `number_of_cakes_ordered` *must* be greater than or equal to zero. `email_address` *must* look something like `x@y.z`, although the specifics can get gnarly.

However, sometimes validations are fuzzier, and the appropriate way to handle invalid data is less clear. Maybe we could probably, with a little imagination, infer what the user meant, but we're not sure whether we should. In such situations, should we be sticklers or swashbucklers? As always, it depends.

Let's look at a stripped-down example, then apply the lessons within to Gianni's breadth-first search code.

## Validating test scores

Suppose that we are writing a program to analyze a school's test scores. We have a function called `analzye_test_scores` that takes in a dataset of scores and uses them to calculate some statistics:

```python
def analyze_test_scores(test_scores):
    # Do some analysis and return some statistics
```

This function expects `test_scores` to be a *nested dictionary* in which the top-level key is the student's name. It expects the nested key to be the name of the test, and the nested value to be the student's score in that test. For example:

```
{
    'rob': {
        'algebra': 100,
        'history': 100
    },
    'sally': {
        'algebra': 12,
        'history': 8,
    },
    # etc...
}
```

However, not every student took every test. There are two ways for us to account for this in our data structure. The most verbose and explicit way is to require that every student have a key-value pair for every subject, but if a student didn't take a test then the value should be `None` (or `nil`, or `null`, or however our language represents "nothing"). For example:

```
{
    'rob': {
        'algebra': 100,
        'geography': None,
        'history': 100
    },
    'sally': {
        'algebra': 12,
        'geography': 7,
        'history': None,
    },
    # etc...
}
```

In this strategy, our `analyze_test_scores` function validates that every student had an entry for every subject, and throws an exception if they didn't. One big reason to prefer this approach is to safeguard against typos in the subject names. Maybe `jerome` doesn't have a score for `science` but he does have one for `sceince`. Our zealous validation code would spot that something was wrong and loudly alert us.

Alternatively we could be more relaxed. We could allow users to pass in datasets with missing subject scores, and assume that any student without a key-value for a subject didn't take the test. This would be more convenient to use, but more open to silly typos. It would probably take us much longer to realize that `jerome` had only taken the test for `sceince`.

Which of these two approaches is best? Sing it with me - it depends.

## Trading off convenience and surprise

Whenever you're writing code that will be used by another programmer (you-in-a-month's-time counts as another programmer), put yourself in the mind of the person using your code. This person doesn't want to have to do much work, and they don't want to be confused. They want to maximize convenince and minimize surprise.

However, these goals are often in tension with each other. The most convenient code is that which requires no instruction and can figure out everything magically and perfectly:

```python
output = analyze_speech_data(
    data,
    input_format="can you just figure it out for me",
    language="mostly english, some spanish i think"
)
print(format_output(
    output,
    structure="the way that frankie did it the other day, that was really cool"
))
```

But with intepretation and inference comes great scope for silly, silent, surprising mistakes. In our test scores example, giving users the freedom to pass in partially filled-in datasets might make their life easier 95% of the time. But the other 5% of the time it saddles them with secretive bugs, or even worse wrecks their results in ways that they'll never notice.

Can we have the best of both worlds? Can we make our library both convenient and unsurprising? Using "configutable validation" and "loader functions" we can certainly try.

## Configurable validation

We could give our users the flexibility to decide how much validation they want us to apply by adding an optional `strict_validation` parameter to our `analyze_test_scores` method. If this parameter is set to `True` then we throw an exception if data is missing. If it is set to `False` then we silently assume that missing keys mean that a test wasn't taken.

Before you read any further - in languages that support *optional parameters*, what should the default value of `strict_validation` be if the user doesn't give us a value?

```python
# If test_data is missing test scores, this will throw an exception
stats = analyze_test_scores(test_data, strict_validation=True)

# This will silently fill in the blanks
stats = analyze_test_scores(test_data, strict_validation=False)

# What should this do?
stats = analyze_test_scores(test_data)
```

I think that the default value of `strict_validation` should be `True`. This ensures that users of our code who don't read our documentation don't accidentally run code with lax validation. Only users who know what they are doing and understand the risks involved will pass in `strict_validation=False`.

## Loader functions

Our library could also provide functions that load test result data on behalf of our users. For example, suppose that users retrieved their data by loading it from a specially-structured file. We could provide a function for loading, processing, and validating this data called `load_test_scores_from_file'. This function might look something like this:

```python
def load_test_scores_from_file(filepath):
    raw_data = open(filepath).read()

    # ...
    # Massage the raw data into the format that the
    # rest of our library expects it, including
    # `None`s for missing test scores.
    # ...

    return formatted_data
```

We could have `load_test_scores_from_file` be responsible for making the same validations and decisions that `analyze_test_scores` curently makes. If data is missing from the source file, this function could be responsible for either raising an exception or filling in the blanks with `None`s. It could even have its own `strict_validation` flag that it uses to decide how fussy to be.

This approach has several pleasing properties. It validates data immediately, at its source, and allows the rest of our program to assume that the data it receives is in the correct format. This simplifies communication between the different components of our program. They're now passing around a dataset that is known to be properly-formatted, rather than a dataset that may or may not be properly-formatted that they have to constantly check and decide how to handle.

In addition, validating data when it's loaded will likely make debugging easier. If a program raises an "invalid data!" exception while it's loading a dataset then it's clear that this is where your bug is. If it instead raises an exception several steps later, when the data is being processed, it's rather more work to trace back the flow of logic to where the data originally came from.

### Other data sources

We could also provide loader functions for loading data from other sources. We could provide functions called `load_test_scores_from_db` and `load_test_scores_from_api` that perform the exact same operations as `load_test_scores_from_file`, but load their raw data from different places. These functions would be responsible for formatting and validating the data, and for returning a dataset in exactly the same form as `load_test_scores_from_file` does.

However, we can only provide standard loader functions for standard data sources. We can provide functions that load data from standardized file or database or API formats, but if our user wants to load data from a custom source or format then there's only so much support that we can give them.

That said, we can still provide a standalone validation function, called something like `preprocess_test_scores`. Our user can use this function in their own data loading code, allowing them to at least be confident that their custom-loaded data is in the right form:

```python
data = load_data_from_my_custom_data_store()
processed_data = preprocess_test_scores(data)

stats = analyze_test_scores(filled_in_data)
```

Users can bring their own data from their own sources, while also using our standardized validation and formatting functions to ensure that their data is in the right format.

A fully-featured library would likely provide all of these tools. It would provide standardized, full-service functions for loading data from standard sources (like files, databases, and APIs), but also provide the building blocks for users to bring their own data from their own sources. In the real world, time and energy are finite, so any library that you write should begin with the highest leverage, most useful tools. You can add additional, edge-case gadgets as and when they are needed.

## How does this apply to breadth-first search?

Gianni's breadth-first search code faces an identical problem to our test-scores code. His `shortest_path` function expects a graph to be passed in in a very specific format. The function expects the graph to be a dictionary in which the keys are the names of the nodes in the graph, and the values are the neighbors of the nodes that can be reached from it. For example:

```
{
    '1': ['2', '3'],
    '2': ['1', '4'],
    # ...etc...
}
```

Gianni's code expects this dictionary to contain a key for *every* node in the graph, even those nodes that have no neighbors. It's possible for a node to have edges that point *into* it, but none that point *away* from it. We can either require that the graph data structure explicitly records the fact that such nodes have no neighbors:

```
{
    '1': [],
    # ...etc...
}
```

Or we can assume that a node's absence implies that it has no neighbors.

This is exactly analagous to our test-scores code. Should we require users to give us scrupulously filled out dictionaries, including empty lists for those nodes without any neighbors? Should we raise an exception if they don't? Or should we infer that a key's absence probably just means that the node has no neighbors, and internally fill in the blanks on behalf of our user?

We have exactly the same options as with our test scores. We can fill in the blanks on behalf of our user when they pass their dictionary into `shortest_path`. We can fill in the blanks when they load their data. We can give them optional parameters that they can use to allow them to choose the blank-handling behavior they prefer.

## In conclusion

Think about how your library looks and feels to use, and in particular think about the tradeoffs between convenience and surprise.
