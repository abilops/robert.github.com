---
layout: post
title: How to save 1 day/year of quality Rails time
published: false
---


Most input validations are trivial checks of whether the input makes any sense at all. `number_of_cakes_ordered` *must* be greater-than-or-equal-to zero. `email_address` *must* look something like `x@y.z` (although the specifics can get gnarly).

However, sometimes validations are fuzzier, and the appropriate action is less clear. Maybe the input as given is not enough for a function to properly perform its job, but with a tiny amount of imagination we can infer what the user probably meant. In such situations, should we be sticklers or imagineers? As always, it depends.

## Validating test scores

Suppose that we have a function that takes in a pile of test scores and calculates statistics for them:

```python
def analyze_test_scores(test_scores):
    # Do some analysis and return statistics
```

This function expects `test_scores` to be a *nested dictionary* in which the top-level key is the student's name, the nested key is the name of the test, and the nested value is the student's score in that test. Its internal logic also expects every student to have a score set for every test. If a student didn't take a test then the value for that test should be `None`. For example:
```
{
    'rob': {
        'algebra': 100,
        'geography': None,
        'history': 100
    },
    'sally': {
        'algebra': 12,
        'geography': 7
        'history': None,
    },
    # etc...
}
```

What should this function do if it receives incomplete input, in which a student's score for a test is missing? For example:

```
{
    'rob': {
        'algebra': 100,
        'history': 100
        # No score given for geography
    },
    'sally': {
        'algebra': 12,
        'geography': 7
        # No score given for history
    },
    # etc...
}
```

We have two main options. We could assume that something has gone wrong and raise an exception saying "invalid input!" Or we could assume that the missing scores indicate that the student didn't do the test, and fill in the missing `None`s ourselves before passing the processed input into the main body of our `analyze_test_results` function. What's the best thing to do? Sing it with me - it depends.

Whenever you're writing code that will be used by another programmer (you in a month's time count as another programmer), put yourself in the mind of the person using your code. Try to maximize convenience and minimize surprise.

These goals are often in tension with each other. The most convenient code is that which requires very little instruction and can do everything magically and perfectly:

```python
output = analyze_speech_data(
    data,
    input_format="can you just figure it out for me",
    language="mostly english, some spanish i think"
)
print(format_output(
    output,
    structure="the way that frankie did it the other day, that was really cool"
))
```

But with automatic intepretation and inference comes great scope for stupid, silent mistakes. In our test scores example, whether we want to infer that missing data means that a test wasn't taken depends on how likely it is that this was caused by an error. How much is the user of our code relying on our function to tell them if there's a problem with their data?
    
There are several ways in which we could try to have the best of both worlds.

We could give the user the flexibility to decide. We could add an optional `strict_validation` parameter to our `analyze_test_scores` method. If this parameter is set to `True` then we throw an exception if data is missing. If it is set to false then we silently fill in the blanks. Before you read any further - what should the default value of `strict_validation` be if the user doesn't give us a value?

```python
# If test_data is missing test scores, this will throw an exception
stats = analyze_test_scores(test_data, strict_validation=True)

# This will silently fill in the blanks
stats = analyze_test_scores(test_data, strict_validation=False)

# What should this do?
stats = analyze_test_scores(test_data)
```

I think that the default value of `strict_validation` should be `True`. This ensures that users of our code who don't read our documentation don't accidentally run code with lax validation. Only users who know what they are doing and understand the risks involved will disable validation.

Another mitigation could be if our library also provides the code that loads and generates the `test_data` variable. For example, suppose that users passed data into our library by loading it from a specially-structured file using a function that we provide called `load_test_scores':

```python
def load_test_scores(filepath):
    raw_data = open(filepath).read()

    # ...
    # Massage the raw data into the format that the
    # rest of our library expects it
    # ...

    return formatted_data
```

We could have this code be responsible for making the same validations and decisions that `analyze_test_scores` curently makes. If data is missing from the given file, it could be responsible for either raising an exception or filling in the blanks with `None`s. It could even have its own `strict_validation` flag that it uses to decide which of these options to use.

If it's a valid option, this approach is in many ways preferable to the others that we have discussed. It validates the data at the source, and allows the rest of our program to happily assume that the data is in the correct format. This simplifies the communication between the different components of our program. They're now passing around a properly-formatted dataset, rather than a dataset that may or may not be properly-formatted that they have to constantly check and decide how to handle.

In addition, it's aesthetically pleasing that the data is validated at the time at which it is loaded. This will likely make debugging easier. If your program raises an exception at the point at which data is loaded then it's clear where you need to look in order to find your bug. If it instead raises it several steps later, when the data is being processed, it's rather more work to trace back the flow of logic to where the data originally came from.

If the standard way of using our library is to load data from a file using the `load_test_scores` function that we provide, what should we do about renegade programmers who want to bring their own data from their own data sources? Maybe they want to fetch their data from an API endpoint, or load it from a database or a differently-formatted file. We have several, non-mutually-exclusive options.

First, we could do nothing. People can do this kind of madness if they want, but we won't support them in it.

Second, we could try to add new loading functions into our library that take care of these people's use cases for them. In addition to `load_test_scores` we could provide `load_test_scores_from_db` and `load_test_scores_from_server`. All of these functions would perform the exact same operations as `load_test_scores`, but would just load their raw data from different sources. They would be responsible for formatting and validating the data, and for returning a dataset in exactly the same form as `load_test_scores` does.

This will only work for standard loading use cases, for example if there is a common database format in which test scores are stored, or a standard online tool that schools use to manage their test results. For users with completely custom data stores, we can't make them a fully functional tool.

However, we can take a third approach, which is to provide a standalone validation function that they can use in their own data loading code. This function would extract the validation and blank-filling behavior from our other functions.

```python
data = load_data_from_my_custom_data_store()
filled_in_data = fill_in_blank_scores(data)

stats = analyze_test_scores(filled_in_data)
```

This approach gives us the useful property of guaranteeing that our data is properly formatted before we start to analyze and process it. It allows users to bring their own data from their own sources, but make use of our standardized validation and formatting functions to make sure that it is in the right format.

A fully featured library would likely provide all of these tools. It would provide standardized, full-service functions for loading data from standard sources (like standardized files, databases, and APIs), but also provide the building blocks for users to bring their own data from their own sources. In the real world time and energy are finite, so any library that you write should provide the highest leverage, most useful tools, and add additional, more edge-casey tools as and when they are needed.

## How does this apply to breadth-first search?

Gianni's breadth-first search code faces an identical problem to our test-scores code. His `shortest_path` function expects a graph to be passed in in a very specific format. In particular, it expects this dictionary to contain a key for every node in the graph. It's conceivable that a node may have directed edges that point towards it, but none that point away from it. It would be reasonable for the dictionary to have no key or value for this node, and for this to imply that the node has no edges pointing away from it.

This is exactly analagous to our test-scores code. Should we require users to give us scrupulously filled out dictionaries, including empty lists for those nodes without any neighbors? Should we raise an exception if they don't? Or should we infer that a key's absence probably just means that the node has no neighbors, and internally fill in the blanks on behalf of our user?

We have exactly the same options as with our test scores. We can fill in the blanks on behalf of our user when they pass their dictionary into `shortest_path`. Or we can fill in the blanks when they load their data. Or we can leave them to fend for themselves. We can give them optional parameters that they can use to allow them to choose the blank-handling behavior they prefer.

A short nitpick to finish: this is a Gianni's code:

```python
graph = read_data(filename)
validate_graph(graph)

path = shortest_path(graph, source_vertex, destination_vertex)
```

The job of `validate_graph` is to fill in the blanks in the `graph` data structure. However, I don't think that the name `validate_graph` accurately describes this job. I expect a function called `validate_graph` to do some validation, and probably raise an exception if the data it is given is invalid. I would prefer this function to be called something like `fill_in_blank_nodes` or the more generic `preprocess_graph` instead.

## TODO

That's all for this week. Think about how your library looks and feels to use. Think about the tradeoffs between convenience and over-inference.









Some validations are trivial - check that data is remotely reasonable
Number >= 0, email address valid

Interesting case where the data is sort of valid and can be massaged into something workable - what should we do?

Principle of Least Surprise - is there any chance that this incomplete data means the input is wrong? Is it surprising that they have to present their data in one specific form?
Principle of Oh God I Can'T Be Bothered - don't require your users to do work for no reason

All about thinking about how it feels to use and read your code

Example with test scores
Should you have to fill in your own Nones?

Two options

Maybe we can go even further. If we control the code that generates the input data, maybe we can fill in everything there

Apply to Gianni's code
Maybe we have a strict=True option or something like that




https://github.com/robert/programming-feedback-for-advanced-beginners/commit/7ca061ea300e53cfa0033670d974a0eb89c86328

We'll talk about validating and pre-processing input, and about what to do when you need your function to return more than one value

Small touches that make a big difference, especially when multiplied many times in every single project
Also get you thinking about how your code feels to use and read******

## Validating and pre-processing input

We talk a lot about *interfaces* in PFAB. Your functions say "you give me X, I'll do Y, and give you back Z." However, sometimes users of your code will give the wrong type of input. Maybe they give you a string when you need an integer (not a problem in *statically typed* languages), or maybe they give you a list containing 2 elements when you need a list containing 3.

Invalid input can cause strange and unexpected things to happen, so it's generally good practice to validate the inputs that programmers using your library pass into it.

Sometimes input is technically invalid, but only because it's in an abbreviated form. For example, suppose you have a function that takes in a dictionary of test scores:

```
{
    'rob': {
        'algebra': 10,
        'history': 8
    },
    'sally': {
        'algebra': 12,
        'geography': 7
    },
    # etc...
}
```

Your function's internal logic requires that every person have a score set for every test, and have `None` if they didn't take the test:

```
{
    'rob': {
        'algebra': 10,
        'geography': None,
        'history': 8
    },
    'sally': {
        'algebra': 12,
        'geography': 7
        'history': None,
    },
    # etc...
}
```

We could have our function validate that every person has a score set for every test, and raise an exception if they don't.

```python
def calculate_averages(test_scores):
    if not all_tests_have_scores(test_scores):
        raise Exception("Some tests are missing scores!")
    # If all tests have scores, perform calculations
```

However, suppose that there are lots of students, and lots of different tests they could have taken. Filling in the blanks might be an annoying chore for our user.

Instead, it would be a reasonable design decision to adjust the function's interface to require a dictionary of test scores, and if a person is missing a score for a test, assume that they haven't taken the test and programmatically fill it in with `None`. Then pass this *pre-processed* dictionary into the main logic of the function.

```python
def calculate_averages(test_scores):
    processed_scores = fill_in_missing_nones(test_scores)
    # Now perform calculation using `processed_scores`
```

Notice that I say that doing this would be 


Sometimes invalid input won't cause a program to break, but it will cause it to behave strangely. For example, what should happen if I try to use my online banking app to transfer you `-$100`? What could happen if the code doesn't have enough input validation?








Validation may prevent 

This may cause strange and unexpected things to happen.






Your functions and components have *interfaces*
