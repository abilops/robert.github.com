---
layout: post
title: HTTP for advanced beginners
published: false
---
Book - more practical examples at end of each chapter. Have Kate say them
Http make my own cliffs notes at the end

TODO - maybe move the thing about HTTP only being responsible for message structure up

Http is stateless
Brief dns

TODO: is this worth including?

Humans use letter-writing conventions to quickly interpret letters. The sender usually puts their own address in the top right, the recipient's address in the top left, the date below the addresses, then "Dear so-and-so", then the body of the letter, then "Lots of love/Yours faithfully/I await your payment menacingly", and finally their signature. If the recipient knows this convention too then they know that the address in the top right is the sender's address, and so this is the address that they should send any responses to, not a random location.

Humans are more flexible than computers. If a sender skips the addresses or opens with "Hey Sir/Madame", the recipient will be able to infer what they meant without much difficulty. Computers are much more literal. This is why we have letter-writing *conventions* that can be trifled with, but an HTTP *protocol* that must be obeyed precisely.

=====


You’ve started yet another company with your good friend, Steve Steveington. It’s an online marketplace where people can buy and sell things and no one asks too many questions. It’s basically a rip-off of Craigslist, but with Steve’s name instead of Craig’s.

You're responsible for building the entire Steveslist platform. However, while you do know how to build small web apps, you have no idea how to build a system capable of scaling to millions of users before your investors notice all the accounting fraud that CEO Steve Steveington will inevitably commit behind your back. You've therefore enlisted your less-good-but-still-mostly-reliable friend, Kate Kateberry, to help fill in some gaps in your knowledge. She's already given you an excellent primer on systems design[LINK], the transcript of which picked up a cool 945 points on Hacker News[LINK]. You don't like to boast, but since internet points can't buy sandwiches or make your alimony payments or fill the yawning void where your self-confidence used to be, boasting seems to be the only use left for those 945 Hacker News points. You think that the marketers call this "social proof".

You found Kate's introduction to systems design very insightful, well-structured, and empathetic. You're hoping that she'll help you continue your journey into the details of software engineering. The next thing you want to understand is the HTTP protocol. You know that web browsers and other applications use HTTP to send and receive data to and from servers. But how? Where precisely does HTTP fit in? And what actually *is* HTTP? Is it a program? Is it a library? How does it relate to all the other technologies and acronyms that you've heard are so important for the smooth running of the internet?

As the Steveslist platform has become more complex, you've started to want and need to care about these kinds of details. You want to design your systems to make full use of the different features of HTTP. You want to use and understand HTTP-based APIs. And you want to use HTTP as an accessible entry point into other subjects that you'd like to learn about, such as lower-level computer networking and how web browsers work.

You especially want to know:

* What HTTP is and how it works
* How HTTP is used in the real world
* The most important features of HTTP
* Why these features exist and how they relate to the real world
* How to work with HTTP in your own programs
* How HTTPS keeps HTTP messages secure
* Case studies of interesting uses and side-effects of HTTP, such as how to snoop on HTTP requests made by applications on your computer or phone

You know just the person who is contractually obliged to help.

----

Kate Kateberry trudges into the Steveslist office in the 19th century literature section of the San Francisco Public Library, ready for yet another day of changing the world and making Peter Thiel richer. Before she has a chance to start her morning ritual of scrolling through Twitter until 3pm, you sidle over to her desk and ask if she can give you a concise, yet detailed, yet real-world-relevant overview of the HTTP protocol. 

She is surprisingly enthusiastic and bounces over to the whiteboard. You wonder if you're paying her too much. Before she can start, you ask if you can skip all the basics about `GET` and `POST` and so on, since you already know all that stuff. You might know that `GET` and `POST` exist, Kate replies, but do you know *why* they exist? Why does HTTP bother with them? Why doesn't HTTP keep things simple and just have a single type of request?

You aren't sure. This, says Kate, is just one of the many nuances that we will learn about today.

You start taking notes. You use your phone because you don't know where your laptop is. You know that it's somewhere in your apartment but you can't find it. You aren't sure if you're allowed to claim on your insurance in this kind of situation.

----

### What is HTTP? begins Kate

HTTP stands for *Hyper Text Transfer Protocol*. HTTP was originally designed by Tim Berners-Lee at the dawn of the internet, and it's the protocol that underlies most of the web. Whenever you use a web browser or an app to load a website, image, or any other kind of data, there's a very good chance that you're doing so using HTTP.

The rules of HTTP describe the structure of messages that clients (like your computer) and servers (like the servers powering this website) exchange with each other.

For example, in order to load this web page, your computer or phone sent an HTTP request to my server that looked something like this:

```
GET /http-for-advanced-beginners HTTP/1.1
Host: robertheaton.com
User-Agent: curl/7.54.0
Accept: */*
TODO
```

This isn't a human-readable summary of the request. This is the exact text (ignoring a few minor complications like encryption) that your device sent to my server. My server parsed and processed this request to work out what you were asking for, and sent back a response that looked something like this:

```
TODO
```

Your device parsed and processed this response, and used it to show you my webpage. We'll go through each component of HTTP requests and responses in more detail later.

### What is a protocol?

HTTP is a protocol. A protocol is a set of rules that allows two or more entities to exchange information. In order to better understand protocols, let's consider a more everyday example: the set of rules for talking on a two-way radio (or walkie-talkie). When talking in a leisurely, face-to-face setting, most humans can use subtle physical and audio cues to help navigate conversational administration, such as whose turn it is to speak and whether both parties have been understood. However, on a crackly, audio-only walkie-talkie much of this human intuition is useless. In this context people become more like computers, and need fixed rules to help structure and clarify their conversations. This is the job of the two-way radio protocol.

The two most well-known rules of the protocol, commonly used and mis-used in spy films, are:

* When you have finished speaking say "over"
* When you are about to terminate the call say "out" (despite what every character in every movie always does, you should never say "over and out")

The protocol has plenty of other rules too:

* Start every call by identifying yourself, for example "Zulu619, this is TangoFoxtrot83".
* If you need the other person to repeat what they just said, say the specific words "say again".
* Do not interrupt when the other person is speaking. If there is an emergency then say "emergency" and describe the problem.
* When you want to say individual letters (eg. "the target's name is Smith, spelt S, M..."), use the NATO phonetic alphabet (eg. "the target's name is Smith, spelt Sierra, Mike...")

Humans can use the radio protocol to communicate clearly with other humans over the airwaves. Similarly, pieces of software can use the HTTP protocol to communicate clearly with other pieces of software over computer networks like the internet. If a properly-written HTTP server receives a correctly-formatted HTTP request then it will be able to understand and parse it, and will respond with a correctly-formatted HTTP response. A system that is capable of using the HTTP protocol is often described as *implementing* it. You could arguably say that an appropriately-trained person "implements the two-way radio protocol", but no one would ever say that in real life.

HTTP is not itself a piece of software; it's a set of rules that are implemented by other pieces of software. You don't "download", "install", or "run" HTTP. The closest thing that HTTP has to physical form are the Internet Engineering Task Force's (IETF) Requests For Comments (RFCs) that define its specification. They are surprisingly readable.

HTTP is an open, non-proprietary standard. This means that it is not owned or copyrighted by any organization, and anyone may create devices and programs that use HTTP. Products created by warring companies can use HTTP to communicate with each other; Apple phones can talk to Microsoft servers running on Amazon hardware. Without common standards you would expect each company to devise their own protocol for talking to their own devices. Some of these protocols might even be considered intellectual property that other companies or hobbyists were forbidden to use. The internet would be a more complex, restricted, and worse place.

### Where is HTTP used?

One of the most prominent places that HTTP is used is for loading webpages. Whenever you open a site, your web browser sends HTTP requests to a server in order to retrieve the site's HTML, JavaScript, images, videos, and other resources. Browsers can sometimes use other protocols too; for example, the *web sockets* protocol is useful for realtime tasks like collaborative document editing. However, as of 2021 most resources in most webpages are loaded using HTTP.

HTTP is also often used to query *APIs*. "API" stands for *Application Programming Interface*, although no one ever writes that out in full. The acronym "API" is used in many situations, but in the context of the internet an API is an interface that allows programs to communicate with an online service. For example, the Facebook API allows programmers to write code that talks to Facebook, instead of requiring a human to click around a browser or an app. APIs may allow programmers to retrieve data (like a list of a user's friends) or write new data (like creating a new status update).

[TODO-PIC] of using an API

Since web browsers are centred around HTTP, websites and their servers must use HTTP to communicate (apart from the odd exception alluded to above). However, if an API or other service is not designed to be accessed from a browser (such as an online dating mobile app) then its creators could choose any other similar protocol. They could even invent their own.

Nonetheless, many still choose to stick with HTTP. HTTP is simple, lots of libraries already exist to support it, and lots of programmers are already familiar with it. As a rule of thumb, if HTTP will do the job for your application, there's a good chance that you might as well use it. On the other hand, some applications, such as online multiplayer games, have particularly tight speed or data constraints that a general-purpose protocol may not be able to meet. Developers of these applications may decide to spend the time and effort designing their own protocol, optimized for their own specific usage.

[[[TODO: maybe move above 2 paras to a later section? feel weird here]]]

[TODO-PIC] of diff between HTTP and your own protocol

There's no difference in the fundamental nature of HTTP whether it's used by a browser, an app, or a custom program that queries an API. HTTP requests are structured the same, irrespective of the type of program that sent them. The data in the HTTP responses might take a different form depending on the context; browsers will often ask to get back HTML webpages, whereas apps and custom programs might require structured data like *JSON*. Even this is a poor dividing line, however, since webpages are increasingly structured as full-featured "web apps" that query the same APIs as mobile apps and custom programs. Nonetheless, in all cases the protocol is still fundamentally HTTP, and it matters little what types of device is on either end of the exchange.

### Where does HTTP fit in?

The HTTP protocol doesn't say anything about *how* its requests and responses should travel between clients and servers. It just specifies their structure. So, how does an HTTP request know where to go? How does a server know where the request came from? How is it possible that an entire movie can be transmitted across oceans and under cities in seconds without losing any of its fidelity or emotional power?

Getting HTTP messages safely from sender to receiver is the job of other protocols, primarily TCP (Transmission Control Protocol) and IP (Internet Protocol). TCP/IP is responsible for moving data between computers, whereas HTTP is responsible for how that data is structured. As a user of HTTP you don't have to worry about the details of TCP/IP if you don't want to. You can use an HTTP library to "send an HTTP request to `steveslist.com`" or set up a server "listening on port 443", and let your libraries and operating system worry about how to actually do this.

[TODO-PIC] of differen layers

This hiding of details is known as *abstraction*. Abstraction is everywhere in the non-computerized world too; we just don't call it that. Think about the abstractions at work when you send a letter. The postal service is responsible for getting letters from A to B; you are responsible for the contents of those letters. You don't have to care how the postal service works after you drop off your letter; the postal service doesn't have to care what's in your letter. The details are hidden wherever possible, and the world is easier to think about as a result.

On the other hand, if you're interested then you can learn about TCP/IP and the myriad other layers underneath and alongside HTTP - Ethernet, DNS, SSL/TLS, and many more. This will be useful and interesting. However, understanding HTTP only requires understanding only the structure, purpose, and sequencing of HTTP requests and responses. It is not essential, although it is useful, to delve into the layers below this. You can leave things at "the client sends an HTTP request to the server" and trust that your tools will do their jobs. 

-----

Nonetheless, says Kate, understanding the lower-levels of a system can't help but improve your understanding of the higher-levels too. You may have found that you can get a long way without knowing much of the details of HTTP, let alone TCP/IP and beyond, but every new layer that you are familiar with will increase your appreciation of the whole. The goal of today's lecture is therefore to give a flavor of the details of HTTP, including seasonings that you won't find in core documentation or brief introductions.

I'm already here, you think, you can cut the sales pitch.

-----

### What does an HTTP exchange look like?

[[[TODO-BABBY - should I structure this section more like "Client sends request to server, server sends back response. Btw this means that you can call it a client-server and request-response protocol." Leading with "HTTP is a client-server protocol." feels kind of off-putting since it puts technical words first]]]

HTTP is a client-server protocol. "Server" and "client" are general purpose words that are widely used all over the field of computing. A server is a system that offers to perform services for other systems, and these other systems are known as clients. A server sits around waiting for incoming messages from clients, and they don't pro-actively reach out to talk to clients (or at least if they do then they aren't acting as a server). When a server receives an incoming message it may perform an action of some sort, and the client and server may engage in an extended exchange of information.

[TODO-pic]

The server for `robertheaton.com` spends most of its time twiddling its thumbs waiting for HTTP requests from a client, such as a web browser. When a client sends the server an HTTP request asking for a resource like a webpage or an image, the server retrieves the appropriate data and sends it back in an HTTP response. It's not hard to come up with real-world analogies to this. A restaurant is a bit like a server; it offers food services, and it sits around waiting for customers (or "clients") to make use of these services. Restaurants don't push food to customers unless they first receive an order.

[TODO-PIC] req/res of sitting around waiting

An HTTP client can be any device that needs to retrieve data from a remote server. It could be a web browser, a phone, a PlayStation, or a smart-fridge. An HTTP server can also technically be anything that is capable of listening for and responding to incoming HTTP requests. In practice servers are almost always big computers sitting in data centers. But if you were determined enough you could probably hook up a website that was powered by your smart-thermostat, and it is entirely possible (if hard work) to run a fully-functional web server from a laptop in your front room.

[TODO-PIC] of phone ignoring incoming requests

In addition to being a client-server protocol, HTTP is also a request-response one. This means that every HTTP exchange consists of a single request from a client and (almost always) a single response from a server. After that, the HTTP exchange is over. If the client wants to request more information then it must send a brand new HTTP request.

It's possible to have a client-server protocol that is not also request-response. For example, the *web sockets* protocol is often used for ongoing, realtime communication between a client (usually a web browser) and a server. This is useful for features like instant messaging. Web sockets still follows a client-server model: servers sit around waiting for connections from clients, and never initiate connections with clients. However, once a connection is established, both parties are free to use the connection to send as many messages as they want, when they want. They don't have to wait for a response, and they can continue to send messages even after they do receive responses. This is very different to HTTP's single request, single response model, and is exactly the behavior you want for an instant messaging application.

You can (and plenty of people do) still use HTTP to create realtime, instant messaging-style applications. However, because every HTTP conversation has exactly one request and one response, the client has to constantly "poll" the server, asking "Any new messages? Any new messages? How about now?" The more realtime you want your application to be, the more frequently the client must poll. This works, but puts a lot of load on the server. By contrast, the long-lived web sockets model allows a server to say "OK client, we've got a connection, it's not going away, chill out and I'll let you know when something new happens."

Different protocols are useful for different things. Web sockets is good at bi-directional, ongoing communication, but HTTP is much more appropriate for downloading a large image file.

[[[TODO: transition]]]

-------

TODO: maybe a Kate section to make it clear that we're transferring from high-level to details?

-------

### Overview of an HTTP request

TODO: should probably mention the different parts of a URL

Clients initiate an HTTP exchange by sending an HTTP request to a server. 

```
GET /http-for-advanced-beginners HTTP/1.1
Host: robertheaton.com
User-Agent: curl/7.54.0
Accept: */*
TODO
```

Remember, this is the exact text that the client sends to the server, not a human readable summary of it. In order to interpret the request the server will parse this exact text, using logic along the lines of "take the first line, then take all the characters up until the first space. This is the *HTTP method* (`GET` in our example). Then take all the characters up until the next space. This is the *request target* (`/http-for-advanced-beginners' in our example)," and so on.

HTTP requests are made up of 5 components. We'll go through each in detail shortly, but in brief they are:

* HTTP method - a parameter that usually implies the "type" of action (eg. return existing data, save new data, update existing data) to be performed. The two most common HTTP methods are `GET` and `POST`
* Request target - the resource that the request will act on (eg. `/about`, `https://google.com/search`)
* HTTP protocol version - the version of the HTTP protocol that the request uses. Most commonly 1.0, 1.1, or 2 (we'll talk about the differences between versions later, although they aren't particularly important for our discussions).
* Headers - metadata about the request (eg. cookies, the type of response expected, data about caching)
* Body (optional) - the main block of data (if any) that the client wants to send to the server (eg. the body of a message to be created, the details of an item to be purchased)

[TODO-PIC] - example HTTP request

Here's an example HTTP request that your browser sends when it wants to request the page `robertheaton.com/about` from my server:

```
GET /about HTTP/1.1
Host: robertheaton.com
User-Agent: curl/7.54.0
Accept: */*
TODO
```

[[[TODO-screenshots, label each part of the request]]]

Here's another HTTP request that an online business might send to the Stripe API. This request is asking Stripe to charge $50 to the credit card of customer `cus_123456789`:

```
TODO
```

Remember, these aren't human-readable summaries of HTTP requests. They're the exact text (ignoring a few minor complications like encryption and HTTP 2) that a client sends to a server, and that a server parses and processes. Let's go through each component of these requests in more detail.

### Request target

The request target of an HTTP request describes the thing, or *resource*, that the HTTP request acts on. The reqeust target is usually a URL path (the part of a URL after the domain, eg. `/http-for-advanced-beginners` for this post), although it can also be a complete URL (eg. `https://robertheaton.com/http-for-advanced-beginners`).

On its own, the request target doesn't fully specify the exact action (for example, create, read, update, or destroy) that the request will perform. Suppose we have a social network that has URLs like `https://socialnetwork.com/posts/123`. Sending an HTTP request with a request target of `/posts/123` will interact with the post with ID 123 somehow, but exactly how (eg. read, update, or destroy the post) will depend on the *HTTP method* (eg. `GET` or `POST` - see below).

Sometimes a request target refers directly to an actual file on the server. For example, one way of running a website is to use a simple server program like the Apache HTTP server[LINK]. You can run Apache on a server and tell it to respond to HTTP requests by looking up the URL path of the request, relative to a root directory on the server's hard disk. For example, suppose that you configure the root directory to be `/var/www/html`. Then, if your server receives an HTTP request for `/images/pineapple.jpg`, the server looks to see if it has a file saved on its disk at `/var/www/html/images/pineapple.jpg`. If it does, it reads and returns the file's contents; if not, it returns a 404 error.

[TODO-pic] how apache works

Note that servers using this approach have to be careful to reject malicious requests for sneaky paths. For example, a request to `GET /../../secret-docs/passwords.txt`, if treated naively, could end up returning the file at `/var/www/html/../../../secret-docs/passwords.txt`. Since `..` means "go up a directory", this path resolves to `/secret-docs/passwords.txt`, which judging by its name is not a file that you want to expose to an attacker.

This simple approach of using paths to directly access a file system was more common at the dawn of the internet, when a website was nothing more than a collection of static files. Nowadays most websites are more complicated and contain mostly dynamic content. These websites run more intricate server software that dynamically generates webpages and other responses on the fly, instead of reading and serving pre-generated files from a server's hard disk. Facebook doesn't have a saved copy of the HTML for your profile page anywhere; instead it dynamically generates it when someone asks for it.

### HTTP methods

Every HTTP request has an *HTTP request method*. Despite the name, a request method doesn't have anything to do with the way in which a request is sent. Instead, a request method is the first word at the start of the first line of an HTTP request. Its job is to dictate the type of action that the request will perform on the resource in the request target. There are many conventions around which HTTP methods should be used for which types of action. As we will see, these conventions provide useful hints to clients about how they should process requests and responses.

For example, `GET` requests usually ask for information about the request target. This information is contained in the response's *response body*, and the form that it takes depends on the context. `GET` requests for webpages that will be displayed in a browser should usually return HTML; requests to an API should usually return data in a more structured format such as *JSON* (more on which later). In a vacuum both are sensible ways to respond to a `GET` request, and it's up to the programmers writing the server software to return data in the most appropriate form.

There are 39 HTTP methods in total[https://www.iana.org/assignments/http-methods/http-methods.xhtml#methods], but if you're a normal person then you'll only ever see or use 9 of these[LINK to MDN] at most. Common HTTP methods include:

[[[TODO: REST gives a normal way to combine method and target]]]

* `GET` - typically used to return information about existing resources. In our made-up website, `socialnetwork.com`, a `GET` request to `/posts/999` should return information about post number 999.
* `POST` - typically used to create new resources. A `POST` request to `/post` should create a new post.
* `PUT` - typically used to update existing resources. A `PUT` request to `/posts/987` should update post number 987.
* `DELETE` - typically used to delete existing resources. A `DELETE` request to `/posts/333` should delete post 333. 

You may have noticed that I've been using a lot of weasel-words like "should" and "likely". This is because all of these "rules" about what different HTTP methods should do are only conventions and best-practices. One of the most common conventions is called REST (Representational State Transfer), which provides programmers with a broad framework for deciding which request targets and methods should cause which actions to be performed on their server. However, there's nothing stopping a programmer from having a `GET` request to `/home` delete a user's account, other than hopefully their colleagues' code review.

Indeed, HTTP request method conventions are routinely broken in situations where the payoff is deemed worth it. For example, consider "magic login links". Some websites don't login their users using a username/password form. Instead they ask a user to submit just their email address. The website generates a long, secret, random token, and saves it to its database alongside the user's ID. The website then sends the user an email containing a "magic login link", the URL of which contains the random login token:

`https://socialnetwork.com/login?token=3873278132h181432hs12d178924nc17982t1dgj9jg89`

When the user clicks on the link, the website reads the token from the HTTP request path and looks up the user associated with the token in its database. Because the token is long, secret, and random, the website concludes that the only way that the sender of this HTTP request could have known about it is from the email containing the magic login link. The website therefore concludes that the requestor is also the owner of the email account associated with one of its users, and so logs the requestor in automatically, without the need for a password.

[TODO-PIC]

Username/password forms are usually submitted using `POST` requests. This is because `POST` requests are conventionally used to create new resources, and logging in creates a new *session* on the website. However, when a user clicks on a link, including one in an email, their browser by default requests the URL as a `GET`. It is possible for a website designer to have browsers request link URLs using other HTTP methods (including `POST`), but this requires JavaScript or HTML forms, neither of which are allowed in emails. This means that, even though magic login links create a new session on the website, they have no choice but to use `GET`. This is a violation of the convention that `GET` requests should only ever return information, and should not "do" anything or cause anything to change on the server. But rules were made to be broken, and magic login links are generally seen as a convenient and reasonably secure way to authenticate users.

HTTP could have been designed without HTTP methods, and with only a single "type" of HTTP request instead. In actual, real HTTP [[[TODO: mention REST?]]], a client would likely use the URL path `/articles` in order to both create and list articles. The server can tell whether the client wants to create or list articles by the request's HTTP method:

* `GET /articles`: retrieve a list of all articles
* `POST /articles`: create a new article

However, one can easily imagine a world without HTTP methods. In this world, all the information about the intent of a request would be contained in the URL path:

* `/articles/list`: retrieve a list of all articles
* `/articles/create`: create a new article

This approach would work. However, HTTP methods and the standard conventions around the types of actions that each method should perform are useful. They provide useful hints to a client about the effect that an HTTP request is likely to have, and therefore how the client should handle requests with different HTTP methods.

Two such hints provided by HTTP methods are about a request's *safety* and *idempotency*.

#### Safety

Some HTTP requests are *safe*. This means that these requests don't create or update any data. For example, an HTTP request that loads a Twitter feed or views a train schedule is safe, because it reads - but doesn't write - data about its target.

Other HTTP requests are *unsafe*. This means that they may create and/or update data, and in doing so change the state of the server. For example, an HTTP request that creates a new Tweet or books a train ticket is unsafe.

It is useful for a client to know whether an HTTP request will be *safe*, because it tells it how restrained to be when making requests. For example, Google uses a tool called a web crawler to continuously load and scrape every website it can find and add the website's contents to Google's search engine. The web crawler (known as *Googlebot*) clicks on links and fires off HTTP requests to servers in order to load their websites. Website owners are generally happy for Googlebot to slither round their site in this way because it helps people find their content.

However, an automated crawler's blind clicking can sometimes inconvenience website owners, for example if Googlebot stumbles across a contact form, or a social media post with a "delete" button that for some reason don't require the user to log in. If Googlebot clicks on those links, it could cause headaches for the website owner.

Googlebot avoids making these kinds of faux pas by only (with [some exceptions](https://developers.google.com/search/blog/2011/11/get-post-and-safely-surfacing-more-of)) clicking on links and making HTTP requests that it believes will be safe and won't change the state of the server. Googlebot can't know for sure whether a request will be safe, because it doesn't know exactly what the server will do with it.

To help, [the HTTP spec](https://datatracker.ietf.org/doc/html/rfc7231#section-4.2.1) defines a convention that all HTTP `GET` requests should be safe (as should much rarer `HEAD`, `OPTIONS`, and `TRACE` requests), whereas `POST`, `PUT`, and `DELETE` requests need not be. Clicking on links to view pages usually makes `GET` requests, so Googlebot can be confident that they will be safe, and can crawl a page and click on its links without worrying that it will cause any problems for the owners.

The fact that `GET` requests are safe is useful in other situations too. When you type a URL into your browser address bar and press enter, your browser always requests this URL with an HTTP `GET` request. Since `GET` requests should only ever read data, not write it, this means that typing a URL into your browser is always "safe". You can be very confident that visiting `https://facebook.com/account/delete` in your browser isn't going to accidentally delete your account.

When you start typing in a URL into your address bar, your browser often shows you a preview of that page, even if you have never visited the page before. Your browser does this by making a pre-emptive HTTP request to the site it thinks that you might want to visit, and using the response to show you a preview. Even though you haven't yet requested the previewed site and might not want to visit it at all, the fact that your browser makes the request as a `GET` means that it can be confident that it is making a safe request that won't cause the server any inconvenience.

[PIC] - of address bar preloading

Without HTTP methods and request safety conventions, there would be no such guarantee.

#### Idempotency

A request's HTTP method also indicates whether the request is *idempotent*. An idempotent request is one for which the effect on the server of sending the request multiple times is the same as sending it once. Like safety, the concept of idempotency is part of the official HTTP spec, but it is also used in many other areas of computing too.

To see why idempotence is useful, suppose that a client sends a request to a server asking to pay £300 for a Nintendo Switch. For some reason, the client doesn't receive a response. Now the client doesn't know whether the request was received and processed by the server but the response was lost on its way back, or whether the request never reached the server at all and so was never processed. The client therefore doesn't know whether it needs to re-send its request. Without further information, both re-sending and doing nothing may be risky. Re-sending could result in making two payments if the first payment was in fact processed. Doing nothing could result in no payments being made if the first payment was dropped. This is why many cautious online checkouts tell you "don't click Submit more than once".

[PIC]

However, this message should not be necessary, and its presence is a sign of a poorly-designed application. If an application is structured with idempotence in mind then there should never be any risk of double-payment, and so a user should be able to attempt to pay as many times as they like. Instead of having its clients ask "plese pay £300 for a Nintendo Switch", the application should have them ask "please pay for the order with ID `31uo3hiuq3enji`. When the server receives a request, if the order has not yet been paid for, the server executes the payment and records it. But if the server sees in its records that the order has already been paid for, it ignores the request. This means that sending 10 requests to pay for a specific order is the same as sending 1.

Many requests are idempotent by default. Any request that is safe (such as all `GET` requests) is by definition also idempotent - if a request has no effect on the server then the effect of sending it any number of times is always nothing! The HTTP spec therefore dictates that `GET` requests should be idempotent.

Some types of unsafe requests can be intrinsically idempotent too, for example `DELETE` requests that delete a resource. If the resource hasn't been deleted, the server deletes it. If it has already been deleted, then it can sensibly ignore the request. The effect of sending the same `DELETE` request multiple times is therefore the same as sending it once. HTTP convention dictates that `DELETE` requests should be idempotent too.

By contrast, `POST` requests are not required to be idempotent (although they can be, and as we've seen from our Nintendo Switch payment example, it is often useful if they are). Because of this, you may have seen your browser display a warning like this:

[TODO-screenshot]

Your browser shows you this warning because you are trying to refresh a webpage that was returned from a `POST` request. Your browser knows that in order to refresh the page, it will have to re-send the `POST` request that generated it. Your browser can't know what the server will do when it receives this re-sent request. But, thanks to the hints afforded by HTTP request methods, it knows that `POST` requests may not be idempotent. It knows that a repeated request may cause a repeated action, and warns you that you might be about to re-trigger an action when you attempt to refresh the page. Contrariwise, your browser will never display this kind of warning for a `GET` request. Without HTTP request methods, it would not be able to make these inferences.

Since idempotence is so useful, some APIs turn non-idempotent requests into idempotent ones using a technique called *idempotency keys*. In this pattern, when a client (such as a browser or another program) makes an API request, it also generates and include a long, random string called an idempotency key:

```
TODO example HTTP req includeing idem key
```

If the client isn't sure whether a request succeeded (perhaps because of a bug in their program, or a problem with their network), the client should send the same request again *with the same idempotency key*. When the server receives an API request that includes an idempotency key, the server checks to see whether it has already processed a request from the user with that key. If it has not, it processes the request as normal and saves a record of the idempotency key that was used, along with the HTTP response that it returned. On the other hand, if the server has already seen the key, it knows that it has already processed this request, and should not process the identical request again. Instead, the server returns the same response that it returned previously. By using this approach a client can make sure that it does not accidentally use an API to make the same non-idempotent request twice.

[TODO-pic] flowchart of idempotence key

For a real-world example of idempotency keys, see the Stripe API documentation[LINK].

[[[TODO-transition]]]

An HTTP request's HTTP method and path describe the action that a request should perform. However, many actions also require additional parameters in order for the server to be able to carry them out.

### HTTP request parameters

We've seen how HTTP paths and methods are usually responsible for telling a server what action it should take (eg. create a user, load all articles, delete a post). However, for many types of action this is not enough information. If a server receives an HTTP request asking it to create a new user then it also needs to know parameters about this new user, for example their email address, password, name, and any other information that the server's application requires. If a server receives an HTTP request asking it to display all of its news articles then it may need filter parameters like author, date range, and category.

There are two main ways of passing parameters like these in an HTTP request: the *query string*, and the *request body*.

#### Query string

A query string is most often used by `GET` requests to send the server extra parameters about the data that it is requesting. It is an optional section of a URL that comes after the URL path and is separated from it by a `?`. For example, in this made-up URL:

`https://newspaper.com/articles?author=RobertHeaton&year=2020`

the query string is `author=RobertHeaton&year=2020`.

Query strings usually store parameters using a `key=value` syntax that looks like `?key1=value1&key2=value2`, although clients and servers are technically free to agree on any syntax that they like. One would expect that a `GET` request to `https://newspaper.com/articles?author=RobertHeaton&year=2020` would return all articles written by Robert Heaton in the year 2020.

TODO: the server can parse these params and then use them, nothing happens magically

TODO: However, the webpage needs to make sure that it sends the parameters in a way that the server will understand. The application running on the server will be written so as to expect to receive parameters in a defined form, and then this form will be communicated to the client. Sometimes this is easy. When a company builds a website, it almost always builds both the frontend webpage and the backend server. This means that it is relatively easy to make sure that the frontend sends data in the form that the backend expects. If the company is building an external-facing API that is called by people outside of their company, they will need to put extra effort into documentation that explains how to send parameters.

`GET` requests should provide all their parameters in the query string. By contrast, `POST` or `PUT` requests should usually pass their parameters in the HTTP *request body*.

#### Request body

A request body is a string of arbitrary text that is attached to the end of an HTTP request. In our previous example of an HTTP request that creates a new user, information about the new user would likely be sent in the request body.

[TODO-PIC]

The HTTP protocol doesn't make any requirements about how a request body should be structured. Servers are free to require data to be sent in whatever form they like, so long as they tell this to their clients. As always, however, there are conventions and standards that most applications make use of. For example, many modern APIs require parameters to be sent using *JavaScript Object Notation*, more commonly known as JSON.

#### JSON

JSON is a *serialization format*, which roughly means that it's a standardized way to represent data in a form (usually a string) that can be easily shared.

For example, suppose that a webpage with a signup form wants to send an HTTP request to a server asking it to create a new user with email `robqheaton@gmail.com` and password `banana123`. Since this request will create a new request the webpage will send the request as a `POST`, with the parameters in the request body.

We've seen how, when exchanging parameters in the query string, clients and servers almost always use the convention `?key1=value1&key2=value2`. They need a similar convention for request bodies in order to understand each other.

Two such widely-used conventions are called 



TODO: Content-TYpe header




The webpage needs to structure its body in a way that the server will understand. This is where JSON comes in. The server might expect 

JSON is by no means the only serialization format available. Older APIs might use *XML*; newer ones might use *protobuf*. You could even invent your own, although I wouldn't recommend that you use it in production.

JSON looks very much like the maps, dictionaries, lists, and arrays that you might use when you're writing a program. For example, suppose that you want to send an API request to create a new article. The API's documentation will tell you how to structure the JSON request body that you send it, including the properties that it needs (such as `title`, `body`, `tags`, and `author`). One reasonable API specification might call for a request body that looks something like this:

```json
{
    "title": "An advanced beginners guide to APIs",
    "body": "TODO",
    "tags": ["programming", "advanced-beginners"],
    "author": {
        "first_name": "Robert",
        "last_name": "Heaton"
    }
}
```

JSON might have the words "JavaScript" in its name, but it can be used in any language. Almost every language has standard libraries for parsing JSON and converting it into that language's own internal data structures. For more information on JSON standards, see json.org[LINK].

TODO: form style?


#### HTTP headers

TODO




HTTP requests are only half of the story. We also need to consider HTTP responses.

------

Your thumbs ache from two hours of frantic note-taking. After the first fifteen minutes all of your notes were typoed nonsense but you kept going because you didn't want to look like you'd lost interest.

Any questions so far? asks Kate. You don't want to seem stupid so you say no.

------

### HTTP responses

When a client sends a server an HTTP request, the server parses the request, performs an action, then sends an *HTTP response* back to the client. This response should tell the client whether their request succeeded and return any data that the client requested. HTTP responses are structured slightly differently to HTTP requests, but they do look similar and share some features. HTTP responses are made up of 3 main components:

* Status line - a standardized status code that describes whether or not the request succeeded. Status codes are written as a number, followed by a human-readable description of the status code (eg. `200 OK`, `404 Not Found`).
* Headers - metadata about the response. Response headers use the same structure as request headers, but their contents are different (eg. new cookies that the client should set, the filetype of the response)
* Body - the main block of data that the server returns to the client. Might be a webpage, an image, some JSON data, a JavaScript file, or something else entirely.

If something goes seriously wrong (eg. the client loses its internet connection, or the server literally explodes half-way through processing the request) then a client might not receive an HTTP response to its HTTP request. However, this should be rare. HTTP is a request-response protocol, and in general every request should receive exactly one response.

#### Example HTTP response

Here's an HTTP request to `example.com`:

```
GET / HTTP/1.1
Host: example.com
Accept: */*
```

And here's an HTTP response to that request:

```
HTTP/1.1 200 OK
Age: 601062
Cache-Control: max-age=604800
Content-Type: text/html; charset=UTF-8
Date: Fri, 13 Nov 2020 08:59:45 GMT
Etag: "3147526947+ident"
Expires: Fri, 20 Nov 2020 08:59:45 GMT
Last-Modified: Thu, 17 Oct 2019 07:18:26 GMT
Content-Length: 1256

<!doctype html>
<html>
<head>
    <title>Example Domain</title>

<...snip...>
```

You can see the status line (`HTTP/1.1 200 OK`) indicating that the request succeeded, then the headers (some of which I've snipped out for brevity), then a newline, and finally the HTTP body containing the HTML that is displayed by your browser when you visit `example.com`.

Let's look at the components of an HTTP response in more detail.

#### HTTP response codes

An HTTP response code (also often called its *status code*) is a number that tells the client what happened to their request. If a request failed, the response's status code describes the broad category of what went wrong. When written out, numeric response codes are often followed by a string describing the code to make them easier for humans to read (eg. `404 Not Found`). Some of the most common codes are are:

* `200 OK`: The request succeeded
* `400 Bad Request`: The server could not understand the request due to invalid syntax or parameters
* `403 Forbidden`: The client is not allowed to access the content 
* `404 Not Found`: The server can not find the requested resource. You've probably seen this in your browser many times. For an example, see eg. `https://robertheaton.com/THIS_PAGE_DOESNT_EXIST`[TODO]
* `500 Internal Server Error`: The server has encountered a situation it doesn't know how to handle. Often suggests that the server's code threw an exception while processing the client's request.

Response codes come in five groups. Groups are often described using a notation like `1xx`. `1xx` refers to the group of status code that begin with a `1`. This covers all numbers from `100`-`199`, although there are far fewer than 100 HTTP status codes that begin with a `1`.

* `1xx`, Informational responses: an interim response sent while a request is being sent. For example, `100 Continue` indicates that the server has received part of the client's request and nothing has gone wrong yet. You will rarely have to deal with `1xx` responses.
* `2xx`, Successful responses: the request succeeded. The exact response code may give some extra color.
* `3xx`, Redirects: a redirect status code means that "further action needs to be taken by the user agent in order to fulfill the request" (taken from the HTTP spec[LINK]). A redirect usually tells the client that the resource has moved to a new URL, and that the client should make a new request to that URL. For example, I've set up `https://robertheaton.com/redirect-example` to return a `3TODO` response that tells the client to retrieve its requested resource by making a new request to `https://robertheaton.com/about`. Web browsers usually follow redirects automatically - try visiting the above link to test this out[LINK].
* `4xx`, Client errors: there is a problem with the request because the client did done something wrong (eg. invalid parameters, non-existent resource)
* `5xx`, Server errors: the server has done something wrong (for example, there was a bug in the server code that threw an exception) and the server is unable to complete the request.

You can read the full list of response codes on MDN[LINK-https://developer.mozilla.org/en-US/docs/Web/HTTP/Status].

[[[TODO: super quick transition? Just to make it clear that we've finished the previous thing?]]]

#### HTTP response body

The HTTP response body contains a blob of text that answers the client's request. Just like with request bodies, the HTTP protocol places no restrictions on how a response body should be structured. The body might contain a webpage, an image, an Excel spreadsheet, or anything else. Textual information like a webpage or JSON is typically sent as human-readable text, and you could probably read and understand this kind of HTTP response.

[TODO-PIC] of intercepting HTML

Files like images are sent as a string of bytes that describe their contents, in the exact same way that they are stored as a string of bytes on a hard-disk.

[TODO-PIC] of intercepting pic

A sensible structure for a response body will depend on the context of the request. We know already that a `GET` request to the path `/articles/123` should return some sort of representation of article number `123`. If `/articles/123` is a website that is intended to be viewed in a browser then the representation should likely be in the form of HTML:

```html
<html>
    <head>
        <title>Article #123 - BIG NEWS</title>

...etc...
```

On the other hand, suppose that `/articles/123` is actually an API endpoint used by programmers inside data analysis companies to retrieve news articles for linguistic analysis. In this case, the response should likely contain a representation of the article in the form of a serialization format like JSON:

```json
{
    TODO
}
```

A company might want to allow its users to view their data through both a webpage and an API, depending on their use case. To achieve this they might have two sets of HTTP endpoints; one for browsers to view articles as HTML (eg. `/articles/123`), and another for programs using the API to view articles as structured data (eg. `/api/articles/123`).

---

It’s 6pm and the library is closing. A librarian tries to ask you to leave. Kate says no and the poor guy clearly doesn't know what to do. He wasn't trained for this.

---

### HTTP 1.x vs HTTP 2

TODO

TODO: equivalent of below section but for HTTP servers

TODO: make it clearer that we're turning a corner and getting to the end. This is all we're going to learn about HTTP strcutrue

### Sending HTTP requests from your own code

Every day you send hundreds, possibly thousands of HTTP requests from our web browsers and other applications. But how can you harness the power of the internet in your own programs? How can you write code that makes HTTP requests? How can you write code that contacts an API, scrapes a webpage, or downloads an image?

Happily, almost all mature, high-level programming languages have pre-existing libraries that can construct and send HTTP requests. All you need to do is give such a library the parameters of your request, such as the request target, the HTTP method, and the target domain or IP address. The library turns these parameters into a well-structured HTTP request, sends the request to your desired destination, and parses the response. This means that you don't have to remember any of the finnickier structural details that we've discussed so far. You don't need to remember whether the HTTP method goes before or after the URL, and you don't have to remember whether headers should be sent in any particular order. You certainly don't have to remember or even know anything about how TCP networking works. This is *abstraction* at work again, reducing the surface area of what you need to think about.

For example, the Python `requests` library allows you to make HTTP requests by writing code that looks like this:

```python
import requests

requests.get("TODO")
```

Libraries like `requests` provide functions and arguments for every part of the HTTP specification: adding parameters, using different HTTP request methods, reading headers, following redirects, uploading files, and all the rest. 

TODO: change examples

```python
import requests

# Send parameters in a POST request body
r = requests.post(
    'https://httpbin.org/post',
    data={'key':'value'},
)
# Set HTTP request headers
r = requests.get(
    'https://api.github.com/some/endpoint',
    headers={'user-agent': 'my-app/0.0.1'},
)
# Upload a file
r = requests.post
    'https://api.github.com/some/endpoint',
    files={
        'file': open('report.xls', 'rb'),
    },
)

# Raise an exception if the request failed
# (status codes 400 and up indicate an error)
if r.status_code >= 400:
    raise Exception(f"Error sending request: {r.status_code} {r.text}")
```

`requests` has extensive documentation[LINK] that shows many more example uses.

You may enjoy trying to write programs that make HTTP requests. It can be very gratifying to successfully communicate with another server and make use of the data that it provides. Search `http client library $YOUR_LANGUAGE`, and read the documentation and examples. Try writing a program that sends and retrieves data to and from an API, for example a bot that pulls the weather forecast for your home town from the [OpenWeatherMap API](https://openweathermap.org/api) and uses the [Twitter API](https://developer.twitter.com/en/docs/twitter-api) to Tweet it. Or you could build a web scraper that politely scrapes information from your favorite websites (search `how to write a web scraper` for inspiration), and publishes it to Instagram (or whatever).

HTTP client libraries are powerful, but sometimes you just want to fire off a quick HTTP request without having to write a whole program. In these situations you may turn to a command line tool like *`curl`*.

#### Sending HTTP requests from the command line: `curl`

Suppose that you want to get a quick and dirty idea of what data a request returns. Maybe you want to see whether passing a new header changes the response you get back from a server, or whether you're even able to connect to a particular URL. If you don't want to set up and write a whole program then you might find `curl` useful instead.

`curl`[LINK] is a command line tool available on most operating systems, including Linux, macOS, and Windows. It allows you to send HTTP requests from the command line by running commands like this:

```
$ curl https://robertheaton.com
```

This command will send an HTTP request to `https://robertheaton.com` (as a `GET` request by default) and print the response. `https://robertheaton.com` is designed to be viewed as a webpage, so a request to it returns HTML:

```
$ curl https://robertheaton.com
<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Robert Heaton</title>
<...snip...>
```

You can add the `-v`, or `--verbose`, flag in order to tell `curl` to print information about the low-level actions it is performing. This can be particularly helpful for debugging when something goes wrong, or for learning about the details of HTTP and networking. You can apply `-v` up to three times for maximum detail:

```
$ curl https://robertheaton.com -vvv
* Rebuilt URL to: https://robertheaton.com/
*   Trying 104.18.33.191...
* TCP_NODELAY set
* Connected to robertheaton.com (104.18.33.191) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
<...snip...>
```

Try running `curl https://robertheaton.com -vvv` from your command-line now. If your shell tells you that you don't have `curl` installed, look up how to install it for your operating system[TODO-LINK]. Then use `curl` to query other URLs, see what you get back, and keep `curl` in mind for when you want to have an experimental poke at an HTTP server.

### Sending artisanal HTTP requests by hand

HTTP libraries like Python's `requests`, `curl`, and web browsers all take care of assembling and sending HTTP requests for you. You will therefore almost certainly go through your entire life without ever needing to construct the raw text of an HTTP request yourself. This is not cheating; it frees you up to focus on the parts of your systems that matter.

That said, it can still be instructive to hand-roll your own requests once or twice. We've said already that the HTTP requests and responses that we've been looking at aren't just human-readable summaries of more complex structures; they are literally the data that a client sends and a server parses. There are complications to this slightly-too-tidy model. The client may use SSL/TLS to encrypt the data before sending it (see the section on HTTPS below), or it might use the newer HTTP/2 protocol which compresses data before sending it. But it is still straightforward, if laborious, to fashion and send your own unencrypted HTTP 1.x requests by hand.

To give this a go you'll need a tool like *netcat*[TODO-LINK]. Netcat is a command line tool that you can use to establish a raw TCP connection with a server. Making a TCP connection between a client and a server is like constructing a drainpipe between the two machines that they can send messages through. Once the drainpipe is constructed, TCP doesn't have an opinions about what the form of those messages should be. That's the job of an application-level protocol like HTTP.

This means that you can use a tool like netcat to open a raw TCP connection to a server, then type out and send your own hand-rolled HTTP requests down your TCP drainpipe. Try this out now. First, install netcat[LINK]. Then use netcat to open a TCP connection to `example.com` - which is a real domain - by running:

```
$ nc example.com 80
```

The `80` tells `netcat` to connect to `example.com` on *port 80*. Ports are a networking-level concept that the HTTP protocol doesn't have to care about much, meaning that right now we don't care either. This command will open a TCP connection with `example.com` and then pause. Once the connection is established, whatever text you type into your command prompt will be sent to the target server over TCP.

Since you have complete control over the data that is sent and are not constrained by the HTTP protocol, you could pass the server non-HTTP nonsense. However, the server will respond politely saying that it has no idea what you are talking about:

```
$ nc example.com 80
how now brown cow

HTTP/1.0 501 Not Implemented
Content-Type: text/html
<...snip...>
```

On the other hand, if you send the server a valid HTTP request, it will receive and process it just as if it had been generated by a program. Try opening a TCP connection and typing in:

```
GET / HTTP/1.1
Host: example.com
```

The server will receive, understand, and respond to your request:

```
$ nc example.com 80
GET / HTTP/1.1
Host: example.com

HTTP/1.1 200 OK
Accept-Ranges: bytes
Age: 275740
Cache-Control: max-age=604800
<...snip...>

<!doctype html>
<html>
<head>
    <title>Example Domain</title>
<...snip...>
```

From the `200 OK` status line you can see that the server received and understood your manually-crafted but well-formed request. It processed your request, and returned the HTML for the webpage that lives at `example.com`. You will never, never need to do this in real life, but trying it out once or twice can be illuminating and can help demystify computer networking.

We've now seen several tools that you can use to make HTTP requests of your own, and this has been interesting and productive. However, in my opinion the real fun is in snooping on HTTP requests sent by other programs running on your computer.

### How to see inside HTTP requests

I have three common reasons that I find myself wanting to spy on a program's HTTP requests. First is because I want to see whether the program is smuggling out any of my personal data (see my posts "Stylish browser extension steals all your internet history"[LINK] and "Wacom drawing tablets track the name of every application that you open"[LINK]). Second is because I want to reverse-engineer undocumented APIs used by websites and smartphone apps, and use these APIs to write my own programs that interact with servers in ways that their creators may have never intended (see my post "How Tinder keeps your exact location (sort of) private"[LINK]). Third is because I want to probe a service for security vulnerabilities (see my posts "Remote code execution vulnerability in KensingtonWorks mouse manager"[LINK] and "Another RCE vulnerability in KensingtonWorks"[LINK]).

Many desktop and mobile apps applications communicate with a central server in order to load, backup, retrieve, and exchange data. Zoom makes video calls [[[TODO: weird]]]. Evernote, a note-taking app, saves your data to the cloud. The Apple App Store loads lists of the latest releases from the Apple servers. These applications are free to talk to their central server using any application-level protocol that they want, or even to invent their own. Nonetheless, many applications stick with HTTP because, as we said at the start of this lecture, it's simple; lots of libraries already exist to support it; and lots of programmers are already familiar with it.

Two common tools for nosing around in other applications' traffic are an *HTTP proxy* and a *network packet analyzer*. Both are powerful yet accessible ways to find out what your computer is doing behind the scenes.

#### HTTP Proxy: Burp Suite

Suppose that you've identified an application that you suspect might be funneling your personal data back to a central server. You want to inspect the HTTP traffic that it is sending and receiving in order to see what data it contains. The first tool to try is a *man-in-the-middle HTTP proxy* (hereafter shortened to *HTTP proxy*), like *Burp Suite*[LINK].

An HTTP proxy is itself an HTTP server, usually (but not necessarily) running on the same machine as the target application. Once you've set up your proxy, you ask the target application to send its HTTP requests to your proxy server, instead of directly to the intended destination. When your proxy receives a request it first logs its contents, and then forwards the request on to the server to which it was originally meant to go. The server sends its HTTP response back to the proxy. The proxy logs the response and forwards it back to the originating application.

[TODO-PIC]

The proxy's logfiles show you the HTTP requests that your applications send and the data they include. When I discovered that the popular browser extension "Stylish" was sending the URL of every single website[LINK] that I visited back to its owners, it was because I was running an HTTP proxy on my computer in order to snoop on the activity of an unrelated application. However, while reading my proxy's logs I noticed some strange HTTP requests being sent to a suspicious domain. A few minutes of investigation later I had worked out what Stylish was up to and had already begun an indignant blog post.

You can also use your proxy logs to work out how your target application structures its requests to its central servers. You may be able to determine the structure of the URLs the application sends data to, what serialization format it uses for its parameters, and what keys and values these parameters contain. The information that you discover may allow you to reverse-engineer the APIs for these central servers, which in turn may allow you to write programs to send your own requests to the servers in order to automate tedious and repetitive tasks like texting your family. Your proxy may provide power features to help with your investigation, such as allowing you to edit parameters in the request or response before forwarding it, or to save a request and repeatedly re-send it.

Proxies only work if you can persuade the target application to send its HTTP requests via your proxy, instead of to their real, intended destination. Some applications provide their own configuration to help with this; others allow you to select a proxy at the operating system level. However, some ignore all requests to send their requests via a proxy. Applications aren't obliged to offer any proxy configuration options, and they aren't obliged to obey the operating system configuration. A technique called *DNS spoofing* may still trick the application into using your proxy, but it can be fiddly and is not guaranteed to work.

It may seem like a good idea for companies to have their products ignore proxies at all costs. Even if a company doesn't have any sneaky data exfiltration to hide, it surely makes their lives easier if no one can reverse-engineer their private APIs or probe their application for vulnerabilities.

However, proxies are not only used by nosy programmers. For example, large, security-conscious companies may force all traffic from their employees' laptops to travel through a proxy in order to detect and block malicious traffic. To ensure compliance, the company may prevent any traffic from leaving the laptop unless it goes via their proxy. In this situation, if an application ignores the operating system's proxy settings then it may find itself unable to contact the outside world at all.

Snooping on programs running on your computer isn't the only use for HTTP proxies. You can also use them to investigate apps running on your smartphone. To do this, you connect your laptop and your smartphone to the same wi-fi or wired network, and then configure your smartphone to send its HTTP traffic via a proxy server running on your laptop. Burp Suite has extensive documentation on doing this[TODO-LINK].

[TODO-pic] of sending phone data via laptop

This allows you to see what data your mobile apps are sending back home (almost always a *lot*), and reverse-engineer the APIs that they use. You may have chatted with someone on Tinder who seemed real and attractive enough, until it slowly became apparent that they were a robot. The robot's controller almost certainly used an HTTP proxy to reverse-engineer the Tinder API, and now uses this knowledge to write programs that make automated matches and send automated messages.

[TODO-pic] of a robotic Tinder account

I would strongly recommend setting up an HTTP proxy and playing around, especially since the community edition of Burp Suite is free[LINK]. Setting up an HTTP proxy teaches you a lot about HTTP, networking, and SSL/TLS encryption (more on which below). It can lead you to some newsworthy discoveries about data privacy, and maybe allow you to cause some light-hearted automated mischief. For inspiration and examples of how I've used HTTP proxies to unmask malfeasance and mess with my mates, see "Stylish browser extension steals all your internet history"[LINK], "Wacom drawing tablets track the name of every application that you open"[LINK], and "Fun with your friend's Facebook and Tinder sessions"[LINK].

[TODO-PIC] of proxy blocking egress

If you find yourself trying to probe an application that isn't respecting your HTTP proxy, or you just want to see a lot more detail about what's happening on your network, you can turn to a *network packet analyzer*.

#### Network Packet Analyzer: Wireshark

HTTP proxies, which we just discussed, work at the HTTP layer. This means that they understand and are deeply integrated with the structure of HTTP requests and responses. They can provide HTTP-specific power-features when analyzing cooperative HTTP-based applications, such as intercepting a request and allowing you to edit it before forwarding it on. However, HTTP proxies are useless for inspecting applications that won't send their traffic through the proxy or that don't even use HTTP.

An alternative way to spy on your programs is to use a lower-level tool called a *packet analyzer*. Packet analyzers look at traffic at the network level. This means that they see and show you all the gory details of every byte that you computer sends out over a network. This includes everything from HTTP application data all the way down to the ethernet control frames that your computer exchanges with your wi-fi router. Even if an application won't co-operate with your HTTP proxy, if it wants to send data over your network, your packet analyzer will see this data as it leaves your machine.

[TODO-pic] diff between proxy and packet analyzer

One popular packet analyzer is called *Wireshark*[LINK]. Since Wireshark doesn't inject itself into the path that data takes out of your machine, it can't provide the same kind of tools to edit requests and responses that proxies like Burp Suite can. However, Wireshark isn't completely ignorant. It understands the structure of a wide range of different protocols, not just HTTP. It uses this knowledge to reconstruct raw bytes into human-readable summaries at the different layers of the stack. For example, it can take the ethernet frames that your computer sends to your router, reconstruct them into TCP/IP packets, then reconstruct the TCP/IP packets into HTTP requests and responses.

[TODO-pic]

This allows it to display the data it captures very usefully in its UI, and allows you to filter traffic using queries like `http.method == GET TODO`. If it didn't understand how to roll up raw bytes into different protocols, all Wireshark would be able to show you is a structureless, meaningless stream of numbers.

[TODO-pic] screenshot of Wireshark

Packet analyzers see everything, but they can't edit or block traffic in the same way that proxies can. They also can't decrypt SSL/TLS encrypted traffic (which we'll talk about more in the next section). This is because the SSL/TLS protocol is so robust. The point of SSL/TLS encryption (and of any other useful encryption applied to data sent over a network) is to prevent eavesdroppers from spying on your traffic. SSL/TLS has the incredible property that even if an attacker watches every single packet of encrypted data that a client and a server exchange, *including the packets in which they negotiate and exchange an encryption key*, the attacker won't be able to decrypt it.

[TODO-pic] exchanging data but not decrping

Packet analyzers watch from the sidelines. From the point of view of decrypting SSL/TLS traffic, it's irrelevant that they run on the same computer as one of the participants in the encrypted conversation. In a very real sense they are spying on your traffic from exactly the same kind of vantage point as an attacker who has hacked into your network. If a packet analyzer were able to decrypt encrypted traffic by watching network packets, so could an evil-doer listening on your coffee shop's wi-fi. Instead, the best that Wireshark can do is to show you the garbled, encrypted bytes and say "there might be something interesting in here but unfortunately I can't tell you what."

[TODO-PIC] of TLS traffic in Wireshark

By contrast, because a proxy inserts itself in between the client and the server, it is directly involved in the encryption process. This means that it is able to decrypt and read the traffic. The proxy negotiates an encrypted connection with the client, and a second encrypted connection with the destination server. When the client wants to send data to the server (and vice versa), the client sends it over its encrypted connection to the proxy. The proxy decrypts the data, logs it so that you can inspect it, then re-encrypts it and sends it along its second encrypted connection with the destination server. This process is known as a *man-in-the-middle*.

[TODO-PIC] of MITM

Download Wireshark and set it running. The output may initially be overwhelming: Wireshark sees *everything*. To filter it to only show HTTP requests and responses, use the filter `TODO`. I made extensive use of Wireshark in my security vulnerability investigation "Remote code execution vulnerability in KensingtonWorks mouse manager"[LINK].

We've spoken about how proxies and packet analyzers do and don't deal with encryption. Now let's talk about how this encryption works.

### HTTPS

[[[TODO-move this above the packet analyzer stuff]]]

The world is a hostile place. You should assume that all HTTP requests that you send might be snooped on or tampered with. When your browser sends a HTTP request containing your username and password to Facebook, this request can potentially be seen by (for our purposes it doesn't matter how) people on your coffee shop wi-fi, your ISP, your government, a foreign government that has hacked your ISP, or anyone else who has managed to insert themselves somewhere in the mazy route that your data takes from A to B.

Secrecy is still possible, however, so long as every important HTTP request that you send is *encrypted*. The current standard encryption protocol is called SSL/TLS, which stands for "Secure Sockets Layer/Transport Layer Security". The product of encrypting HTTP traffic with SSL/TLS is known as HTTPS; the "S" is for "secure". HTTPS is becoming so ubiquitous and straightforward to implement that not using it is commonly considered a security vulnerability. [[[TODO - if an attacker intercepts encrypted traffic then they can't read it.]]]

As well as keeping HTTP traffic safe from eavesdropping and tampering, SSL/TLS and HTTPS also allow a client to verify the identity of the server that they are talking to (and, more rarely, vice versa). When a client attempts to negotiate an encrypted connection with a server claiming to be `facebook.com`, the client will require the server to present an SSL/TLS *certificate*. The certificate must attest that the presenting server is the real controller of `facebook.com` and must be *cryptographically signed* by a trusted third-party called a *certificate authority* (CA). A CA should only issue a signed certificate for a domain once it has verified that the recipient does indeed own and control that domain. Certificates allow a client to be confident that the server they are talking to is the real `facebook.com` or `gmail.com`, and not an attacker pretending to be one of these services. This is important because it doesn't matter how mathematically bulletproof your encryption algorithm is if you exchange your encryption keys with a hacker. I've written much more about certificates in my post "How does HTTPS actually work?"[LINK].

We said a paragraph ago that "you should make sure that every important HTTP request you send is encrypted". However, you don't directly get to decide this. Your web browser will encrypt its conversations with any server that offers SSL/TLS, but if a server does not offer SSL/TLS then you can't unilaterally encrypt your requests and expect the server to figure things out. Instead your browser will send its requests in unencrypted plaintext and probably display a sad open padlock in the address bar.

[TODO-PIC] screenshot of padlock

Being pragmatic, it's probably safe enough to browse someone's personal blog over plain HTTP, but if your online bank doesn't offer HTTPS then you should find yourself a new bank. Similarly, the applications that you use on your computer and phone "should" encrypt communications between your device and their servers. However, if they don't then you'll probably never know unless you go snooping and you don't have any options other than accept the risk or stop using the application.

On the better-behaved side, many websites automatically redirect unencrypted HTTP requests to their HTTPS equivalent using the 301 HTTP status code. For example, try using your browser to visit `http://robertheaton.com` (note the `http`, not `https`, at the start of the URL). You will be redirected to the equivalent HTTPS URL at `https://robertheaton.com`. You can also make this same request to `http://robertheaton.com` using `curl`, and see the raw HTTP redirect response:

```
$ curl http://robertheaton.com
<...snip...>
HTTP/1.1 301 Moved Permanently
Location: https://robertheaton.com/
<...snip...>
```

Many APIs will only accept HTTPS requests, and reject unencrypted HTTP:

```
$ curl http://app.asana.com/api/1.0
The Asana API can only be reached via HTTPS
$ curl http://api.stripe.com
{
  "error": {
    "message" : "The Stripe API is only accessible over HTTPS.  Please see <https://stripe.com/docs> for more information.",
    "type": "invalid_request_error"
  }
}
```

We've already seen how the network layer and the application layer are cleanly separated from each other. The SSL/TLS encryption layer is similarly separated from both these layers too. Adding SSL/TLS encryption to an HTTP conversation doesn't change anything about the HTTP layer. When sending an HTTPS request, the client opens a TCP connection with the server and forms its HTTP request exactly as normal. Then, instead of sending the request straight off to the server, the client first passes it through an encryption algorithm. Using the algorithm first requires some negotiations with the server - carried out over the existing TCP connection between the client and server - in order to agree on an encryption key. Finally the client sends the now-encrypted request to the server.

[TODO-PIC]

When the server receives the encrypted request it starts by decrypting it to recover the original plaintext HTTP request. It can then process the request just like a plaintext HTTP request, without having to care that it was ever encrypted. When the server comes to send an encrypted HTTP response back to the client, the exact same process is followed but with roles switched.

I've written much more about the details of how HTTPS works in my posts "How does HTTPS actually work?"[LINK] and "HTTPS in the real world"[LINK], in which we go deep into the details of TLS/SSL and public key cryptography.

----

FINISH FINISH CONCLUSION

You ostentatiously start converting your notes into flashcards, or at least you pretend to, since your notes really do read like a Mark Rothko painting by this point. You start explaining the benefits of flashcards and spaced repetition to Kate. Oh my god I do not care, she says.

Your phone runs out of battery so you give up for the day. Is that it? you ask. That depends what you mean by "it", replies Kate. It's a very good introduction to the HTTP protocol and an excellent placement of the protocol in the real world. If you understand everything that we just talked about then you'll be well-placed to work with the details of HTTP.

On the other hand, there's an entire career's-worth of extra stuff you can learn about. You could go up the stack and learn about how web browsers work. TODO: Security model is interesting.

You could go down the stack and look further into all the acronyms that we glossed over: TCP/IP, DNS, SSL/TLS, and so on.

Or you could go even deeper into the details of HTTP itself, perhaps learning more about some of the more commonly-used HTTP headers. Cross-Origin Resource Sharing (CORS) is interesting. You could look into the differences and similarities between HTTP 1.x and HTTP 2.

I really do recommend that you start by setting up Burp Suite or another HTTP proxy. It's a great way to get your hands dirty with some of the concepts we've discussed today. It's interesting to see what data your programs, websites, and smartphone apps are sending themselves. It's very fun to reverse engineer non-public APIs and send them your own requests. And it's fun to try to probe these apps for security vulnerabilities. Suppose you intercept an HTTP request from a note-taking app asking to load all the private notes for user number `12345`. What happens if you use your proxy to change that to `12346`? Hopefully the app's server does some authorization to see if you should be allowed to load that user's notes, but everyone makes misakes. The lower-quality and more rushed the app, the more likely an interesting vulnerability awaits for you to find *and tell the app's creators about*, like a responsible citizen [[[TODO: change]]]. I'd recommend you give Wireshark a try too, although I think that it's rather more intimidating.

Since your phone is dead you ask Kate if she'll call you an Uber. She declines, pointing out that you always pull this crap and never pay her back. You don't know how public transport works in this city so you begin the lonely trudge home to the Outer Richmond.





In the most literal sense 








==============


### TODO POSSIBLE EXTRA TOPICS

* What is localhost?
* Where do cookies fit into this? (this is a good one)
* Difference between HTTP 1.x and HTTP 2
* What does it mean to listen on a port?

### TODO EXTENSIONS

* Use Postman
* Write your own HTTP client


Deleted section:

you can usually muddle through the tasks in front of you, using libraries and hand-waving to get the job done, after a fashion.


### Conventions and normality

Technology can be very flexible. The internet and the protocols that go into it are deliberately designed to be extensible for new use cases, and to allow different systems written in different languages by different companies to talk to each other.

Despite this flexibility, in practice most systems are built by piecing together the same components in the same ways. Servers can be anything, but they are usually racks of computers in purpose-built data centres. You can technically send HTTP requests via any communication medium that you like, but in practice you will always use TCP/IP. In general it's a good idea to follow these common paths unless you have a specific reason not to. Sometimes this is because the common paths are very good. Sometimes the common paths might be unfortunate accidents of history that make the world a bit worse, but it's still convenient to do whatever everyone else does.

Many choices are driven by flexible tradeoffs, not rigid technical requirements. The answer to "why can't I do X?" is often "well I suppose you could, but Y will probably work a little better." "Why can't I send my IP packets using carrier pidgeons?" "Well I suppose you could[LINK], but most people just send them over the internet."






A request target usually only needs to include the path (the bit after the `/`), and not the full URL (eg. `http://newspaper.com/articles/123`) because routing an HTTP request to the correct server on the internet is the job of the TCP/IP protocols, not the HTTP protocol.



API requests to create or update resources will also usually return the newly-created or updated resource to give the client full information about the operation that was just performed. We might expect a request to `POST /articles` to create a new article, and the HTTP response might look something like:

```json
TODO
```

If there was an error - say, the user didn't have the required permissions - then the response's status code should indicate a problem and the response body may give extra debug information (although it is not obliged to):

```json
{
    "success": false,
    "error": "Insufficient permissions"
}
```






To use an HTTP-based API, a client sends the server an HTTP request describing the action that it wants the server to perform. The request usually includes an authorization key that verifies the client's identity. The server performs the action, and sends back an HTTP response.


HTTP is like the rules for writing a formal letter. The letter-writing-protocol is a set of dictums for laying our a formal letter. You put your address in the top right, then the date, then "Dear So and So", then I don't know, who actually writes letters any more? The person receiving a letter also knows about the letter-writing protocol. They know that the lines in the top right are your address, and the date is your address. They know that if their name is Ms. Wrigglesworth and the letter is addressed to Mr. Clumpsford then something has probably gone wrong.





SImialrly, The letter writing protocol isn't a thing - it's an idea. It's written down in XYZ, and people use it to communicate with each other. Letter writing protocol isn't so important for humans - humans can easily fill in mistakes. But imagine you had two machines writing to each other. They need names, address, etc, to be in the exact right place. This is where HTTP is important.



The word "server" is often used to describe both the piece of software that makes services available, and the physical computer that this software runs on. 

The two-way radio protocol is a set of rules that allows humans to communicate over (almost always) a radio. In the same way, HTTP is a set of rules that allows applications to communicate over (almost always) an internet connection. I included those "almost always"-es because there's technically nothing stopping you using the two-way radio protocol to talk to your parents across the dinner table, and there's technically nothing stopping you sending HTTP messages to your friends via carrier pigeon. It's just that 




If you have a working understanding of the core of each of the above components then you'll understand most of HTTP. If you encounter any nuances or edge-cases ("how exactly do *CORS headers*[LINK] work again?") then you can research them on-the-fly. Very few people have, and no one should expect, an encyclopedic knowledge of any protocol. When you do need to sweat the details the Mozilla Developer Network (MDN) docs[LINK] are very thorough yet approachable.




A request target often refers directly to a tangible or abstract "object", such as an image, a user profile, or a collection of comments. However, it doesn't have to.


For example, my "About Me" page is at `robertheaton.com/about`. There's no such thing as an "about" object, although you could argue that this request refers to a page object that is called "about". Either way, this fine distinction isn't important. What is important is that, while a request target often clearly refers to an object, there are no technical rules or restrictions. A server is free to respond to requests for any request target with any type of information.






SS/KK

You ask if you can have a lunch break

You ask if you can have a bathroom break

You steeple your fingers in a manner that you hope can be described as “thoughtful”.

Your laptop is out of battery





You take notes on your phone because you aren't sure where your computer is. You 
Your phone notes are garbled and make no sense but you don't want to look like you've lost interest
You ostentatiously start making flashcards


You want to look clever
You want to cover up looking like a dummy

You cancel your singing lessons





KK wants to help





Because HTTP is a client-server protocol, HTTP conversations only happens when a client (like your phone or computer) initiates one. For example, when you click on a profile in the Facebook app, the app sends requests asking for the data it needs to populate the profile page.

[TODO-PIC] 

Since your phone doesn't act as an HTTP server, it doesn't listen for incoming requests in the same way that Facebook servers do. This means that Facebook can't contact your phone until your phone contacts it. Even though Facebook is famous for smuggling out personal data from your devices, they can only do this by writing code in their app that pro-actively sends it to them. The Facebook app or website has to grab the data that it wants, and then siphon it off by sending an HTTP request to the Facebook servers. The Facebook servers can't reach out and ask for the data; they have to wait until it is sent to them. Facebook gets your data either way, but from a technical point of view the distinction is important.
