

Last time on Programming Feedback for Advanced Beginners[TODO] we started speeding up Justin TODO's ASCII art generator. We looked at how to improve the algorithm Justin was using to map image colors to terminal colors, and devised an approach that got the job done ten times faster.

A ten-x speedup is nothing to sneeze at, but this week we're going to optimize Justin's program even further. It might feel like what we're going to do is cheating, but it's really just good pratice. If you haven't read [last week's PFAB][TODO] then I'd suggest you start there. It's not long and it got rave reviews.

## Precomputation

At the moment Justin's program goes through each pixel in a source image and computes the closest ANSI color[TODO] that can be displayed by his terminal. It does each calculation as it goes, on-the-fly. But what if we already knew all the answers? What if we had already worked out the closest ANSI color to every possible pixel color? Then our program wouldn't need to do any computation, and could instead instantly look up the closest ANSI color to each pixel color in zero seconds flat.

[TODO-PIC]

"Well yes," you might say, "if you know all the answers in advance then of course your algorithm will be faster. But there's no such thing as a free lunch, and we still have to do all the work to find those answers at some point.

"In fact," you might continue, "if we use this approach then the price of our lunch actually *increases*. If we use the on-the-fly method method for an 80x80 image then we have to calculate the closest ANSI color to a pixel color `80 * 80 = 6,400` times. But if we calculate in advance the closest ANSI color to every possible pixel color then we have to do `256 * 256 * 256 =~ 16,000,000` calculations, almost all of which will be wasted. This is like saying that producing Lord of the Rings was easy because all the publisher had to do was press print and a best-selling book came out." If you were paying me for any of my blog posts, which you aren't, you might ostentatiously demand a refund.

However, you'd be missing some critical subtleties. What we think of as a program's "speed" depends on the context. Programs are written to be used, and so arguably the most important definitions of "speed" is a measurement of the time that a program takes to respond to a user's request. For our program, this is the time between a user asking for a colorful ASCII art image and our program printing it to the terminal. 

The precomputation approach may usually require more total work to be performed. However, we can do all of the precomputation in advance of the program being run. We can store the results in a file or a database, and load them when our program runs. The user doesn't have to sit and wait while we run all thes calculations. There's no free lunch, but like any good investment bank we can use some sleight of hand to palm the costs off onto someone else.

### Is this actually a good idea?

Whether this is actually a good idea depends on the specifics of how Justin intends his program to be used and how he is able to store a precomputed color mapping.

[TODO-PIC]

The two simplest ways to store a precomputed mapping are in a file using a *serialization format* like JSON, or in a database like SQLite.

JSON:

```
TODO
```

SQLite:

```
TODO
```

As always, there are tradeoffs between these two approaches. I would expect the code that loads data from a JSON file to be simpler than the code that loads it from a database, because there are fewer moving parts. I would also expect it to be faster to retrieve a record from a JSON file (once it has been loaded and parsed) because retrieving data from Python dictionaries is very fast. Retrieving data from a database, especially one with 16MM rows, is typically slower than *in-memory* operations, although can be sped up by using *database indexes*.

On the other hand, at the start of our program it will take much longer to load and parse a large JSON file than it will take to connect to a SQLite database. I generated a JSON file containing all the precomputated mappings from RGB values to the closest ANSI colors and it was 350MB in size. I think I could have used some tricks to slim it down (which we'll talk about in a future PFAB), but not by very much. My computer took around 20 seconds to load and parse my file. For a single 80x80 image, this means that even though generating an image would take a fraction of the time as the on-the-fly approach, the overall program would still take substantially more time to run from start to finish.

This long list of tradeoffs is why the way in which Justin's program is used is important. If the program is only intended to generate a single image at a time, then storing our pre-computed color map in a database will likely be the best solution. This is because it's not worth spending the time to load a large JSON file if we're only going to use it once. However, if the program is used to generate many images in a single run then we still only have to load the JSON file once. This means that accepting the large fixed cost of loading the JSON file starts to look like a good idea if it reduces our time to retrieve the color mapping for a pixel.

[TODO-PIC]

Let's formalize this a little. The overall running time of our program can be estimated using the following equation:

```
total_time = startup_time + (time_per_image * n_images)
```

When `n_images` is small, `total_time` is dominated by the `startup_time` term. This means that to minimze `total_time` we need to focus on minimizing `startup_time`. This indicates that we should read our data from a database, or even stick with calculating it on-the-fly as in the previous PFAB[TODO].

However, as `n_images` gets large, the `time_per_image * n_images` term starts to dominate. Now to minimize `total_time` we need to focus on minimizing `time_per_image`, not `startup_time`. In other words, the more images we process, the more sensible it becomes to accept a large `startup_time` if we get a smaller `time_per_image` in exchange. This is exactly what precomputing all the answers achieves, especially if we load it all into memory from a JSON file.

Using an approach with a small `time_per_image` also becomes more beneficial the larger each image is. This is because the more pixels we are processing, the more important it becomes to minimize the time we spend on each pixel. We can rewrite our previous equation in terms of pixels as follows:

```
total_time = startup_time + (time_per_pixel * pixels_per_image * n_images)
```

The larger `pixels_per_image` is, the larger the second term, and the more important it becomes to minimize `time_per_pixel` instead of `startup_time`.

### Getting the best of all worlds

The fastest approach depends on the inputs our program receives. This means that if we really, *really* want to optimize our program's performance (which after all is the whole point of this post) then we should change our algorithm based on the inputs we receive.

We could run our program repeatedly and record how the different algorithms perform for different inputs. We could use this data to estimate how long each approach will take for a given new input. For example, if we are given 1000 images that are each 640x480 pixels in size, we might choose to accept the startup performance hit of loading the precomputed color map in exchange for a minimal time-per-pixel. We have so many calculations to do that we will be able to efficiently *amortize* the startup cost over them all. By contrast, if we are given just 2 images of 80x80 pixels in size, we might decide that connecting to a database or even using an on-the-fly approach will be quicker. All of this calculation can be done transparently in the background, without the user having to know or care what we are doing.

### Where's the code?

I've updated Justin's program to precompute color mappings and store them in a JSON file. You can read the full code [here][full-code-TODO], or just the diff between the old and new versions [here][diff-TODO]. Storing color mappings in a database and choosing an algorithm based on user inputs is left as an exercise for the reader.

## Conclusion

At first glance, precomputing the closest ANSI color to every single RGB value seems like a stupid, gluttonous idea. And in some situations it is. But what makes sense for a single image might not make sense for hundreds of images, and the more work your program is doing, the more work it's worth you putting in to optimize it.

You'll rarely need to write out equations in the same way as we did here, or to know off-hand how long it takes to access a dictionary or a database. But it does pay to build up your intuition about how different approaches behave at different scales. And if you need to cast-iron ensure that your program is as fast as it can possibly be then you can't beat trying a few things out and simply timing them.




as the scale of our problem increases from one to many to many, many images, intuitions about what 





 We can perform this calculation







We could give the user the option for which algorithm they want to use. We could even estimate how long each approach will take, and choose the fastest one.


Do you want people to have to download a giant database when they install your program?

This approach sounds stupid, but is actually kind of smart in some circumstances





Instead of finding the closest destination color to each source color on our image on-the-fly while we are analyzing the image, we can precompute the closest color for every possible source color before we have even loaded the image. We store all of this data in a map (or dictionary or hash or whatever your language calls it). Then, when we analyze the image, we don't have to do any calculations. Instead we just look up the current source color in our map, immediately finding the appropriate destination color.

Since we do all our calculations ahead of time, we don't have to do any heavy lifting while we are analyzing the image. This means that the time between loading the image and outputting a colorful ASCII-art version of it is shortened.

"But -" I hear you say "- we've just swept all the hard work under the rug! We still have to calculate the closest color `256 * 256 * 256 = 16.7MM` times. Surely we can't just pretend that all that work never happened? That would be like saying that producing Lord Of The Rings was easy because all the publisher has to do is press print and they've got themselves a book."

This is a good point, and serves as a good reminder that there is no such thing as a free lunch. However, it doesn't mean that we can't structure our lunch more efficiently so as to suit our particular requirements. It would be foolish to run this precomputation every time we run our program. However, we can instead run it once, then save the results in a file or a database for future reuse. We pay a very large up-front cost, but get to reuse the fruits of this work to speed up every colorful ASCII-art that we create.

This tradeoff won't always be worth it. If you only want to produce a single ASCII art picture then there's no point doing all this up-front work. But if you have an entire Dropbox account of Christmas pictures that you want to display on your terminal then you get to *amortize* the cost over a larger number of iterations.

## Caching

Sometimes full pre-computation isn't feasible. Perhaps the space of possible inputs is too large. In this situation you may want to instead use caching.

Caching is somewhat similar to precomputation. However, instead of precomputing every possible value before analyzing your image or images, you start analyzing immediately. But every time you find the closest destination color for a source color, you make a note of that mapping in your cache for future reference. The first time you try to map a source color, you have to do the calculation in full. But the second and subsequent times you can reuse the value you calculated the first time.

This has the advantage over full precompuation that you don't need to do any up-front work. The disadvantage is that it takes longer to process your image, especially if it doesn't feature many repeated colors. This disadvantage can be mitigated if you save and reuse your cache between images. As your cache gets fuller and fuller it starts to resemble the fully precomputed store.

How important these factors are to you depends on your specific application and your requirements. If you're just tinkering around on your own computer and just want to speed up your overall execution time a little then caching might be the most prudent approach. The program feels and runs very similarly to the non-optimized version, but doesn't do the same calculation twice.

However, if you're providing an online colorful ASCII-art-generation service then consistent speed becomes more important. You don't want any of your users to have to wait for their images, even those who make the first requests before your cache is highly populated. You might be more inclined to use pre-computation.

## Other examples of caching

### Photomosaics

In my *Programming Projects for Advanced Beginners #4: Photomosaics*[LINK], the reader takes a source image and converts it into a photomosaic, built using a large dump of mosaic iamges. This is a very similar challenge to producing colorful ASCII art. Instead of mapping sections of the source image to the most similar terminal colors, we need to map the sections to the most similar mosaic image.

This is slower than colorful ASCII art. For each section of the source image we have to:

1. Calculate the average color of the source image
2. Calculate the average color of each mosaic image
3. Find the mosaic image that is closest to the source image

Without any optimizations, the photomosaic program takes several minutes or more to run. This has an appreciable impact on the quality of life of the user, which means that it is acceptable and indeed sensible to try to speed the program up.

We can speed up the program using the same techniques we've discussed for speeding up the ASCII art program. We can even apply the techniques at two different levels.

First, we can reduce duplication of work in step 2 above - "calculate the average color of each mosaic image". Instead of re-doing these calculations for every source image, we can do them once and reuse the same results for each section of the source image. We're going to have to calculate the average color of every mosaic image for the very first source image section, so there is little practical difference between pre-computation and a cache.

Second, we can cache our results in step 3, in exactly the same way as for our colorful ASCII art. We can precompute the closest images to each RGB value, or compute the closest images on the fly and re-use our results when we next see the same source image color. If we want to avoid re-using the same mosaic image in our output image then we can return the top few closest matches and have the mosaic-building code randomly choose between them.

Once again, which approach is best depends on what we're trying to achieve.

### Twitter

I haven't been keeping up with exactly how Twitter generate user timelines, but I know that there's a lot of optimization involved. The naive version might look something like "select all tweets by all users that the current user follows, then order them by time created." This works for small numbers of users, but once you start to have a large number of users creating a large number of Tweets, it starts to become infeasible to run such finicky queries regularly.

Instead, Twitter generates and stores user timelines in advance. Whenever a user Tweets, a copy of the Tweet is inserted into the timeline of every user who follows that user. This effectively precomputes timelines; now when a user loads their timeline, Twitter doesn't have to scrabble around to collect all the Tweets by users that they follow and interleave them in the right order. Instead they can simply load all Tweets in the user's precomputed timeline, no extra work required.

I can't find the articles in which I read this, but I promise that they did exist. Even if they don't and I'm totally wrong, this would be a reasonable architecture for Twitter to use.

## Real-world complications

Our ASCII-art project is a particularly amenable to caching, because the calculation we are doing is a *pure function*. This means that it depends only on its inputs, and doesn't have to load any other data from an external source like a file or database. If the source color `(31, 42, 49)` maps to the destination color `(30, 40, 50)` when the function is first invoked, it will always map to that color from now until the end of time. A task like "give me all the comments on this post" requires more care and consideration of tradeoffs. How important is it to always give the exact, up-to-date, guaranteed-correct answer? Is this more important than saving a few milliseconds here and there? These tradeoffs will be different when considering social network posts and bank balances.

We are also lucky that the amount of data that we want to store in our cache is not that large. Suppose that we precompute every mapping from every source color to a destination color. Let's say that storing each mapping takes about 10 bytes. So storing the entire cache will only require `256 * 256 * 256 * 10 = 167MM bytes = 167MB`. This is not an insignificant amount of data, but it's easy for a modern computer to store the entire cache in memory.

Suppose instead that we were storing much more data in our cache. We only have a limited amount of memory that we can store our cache in, so eventually we'll have more data than we can fit in our cache. At this point we'll have to start making decisions about which data is most important to keep in our cache. There are many different strategies we can use to decide which data to *evict*; some commons ones are to evict the *least recently used* data first, or the *least frequently used*.




the speed of every image is important, even those request




Reader sent me code
Asked how to make it faster

First response to this should always be "why do you want to do that?"
His program runs in 0.001s, don't need to shave off 0.0001s
Make code faster if its speed is causing you problems
cf Photomosaic project - takes ages, makes iterating difficult
TODO: where else would this be useful in real life?

Faster code is often more complex
"Premature optimization..."
I believe that Knuth was talking about ridiculous optimizations that make your code harder to read
If you can make your code faster without making it gnarly, then OK fine

Any code that runs in the background doesn't need to be particularly fast
Might as well be as fast as you can
But I'd rather spend time and energy making it reliable, easy to read, etc

The general problem is "given a point and a set of other points, which other point is the closest?"
Then repeat this over and over again

The correct approach depends on your situation and other constraints
Not just in the sense of is this worth spending time on - in the sense of what is the best end state?

There's probably a fancy fast algorithm that speeds up the calculation
This might be the kind of optimization that Knuth was cautioning against
Could approximate by just finding L1 distance
Not generally applicable, not what I want to talk about

Generally applicable processes are pre-computation and caching
We're going to assume that we are dealing with integers. You'll see why

## Pre-computation

For every 256**3 combinations we calculate the closest ANSI color
Make a big dict
Then when we want to find closest color we just look it up in our dict
Dicts are O(1) to access, so this is very quick

But isn't this mad, we did 256**3 computations at the start??
Yes, but we only did them once. We can save their outputs and then reuse the results in every photo we do in the future
Could even do calculations to 0.1, 0.01 accuracy if we wanted

The limiting factor probably becomes loading the cached results
If we had 100000000000 elements then loading them all into memory at the start of the program would take ages
This still might be worth it! If we were processing loads of photos at once we could *amortize* this cost over them all
Unlikely to be worth it for a single photo

Maybe we would be better off loading the data into a DB
Maybe then accessing the DB takes longer than just doing the calculation!
This is more worthwhile the more expensive the original calculation is

## Caching

Don't pre-compute everything, but every time you do a calculation cache it so you can reuse the result if you need to
Useful if there are lots of repetition and you can't precompute everything for some reason. Maybe the space is too large
Could save your cache between images if you wanted

## So what does the code look like?

(Maybe next week? This could get gnarly)

Write to the same interface so it's easy to swap in and out
Probably use a class

```python
# Don't technically need to mention colors. How can
# you make this as generic and general as possible
# without doing anything stupid?
class ClosestPointFinder(object)

    def __init__(self, points):
        self.points = points

    def find(self, source_point):
        min_point = # ETC
```

```python
class MemoryCachedClosestPointFinder(object):

    def __init__(self, finder):
        self.finder = finder

# TODO: we can even abstract out the cache and the finder
```

Generic function cacher!