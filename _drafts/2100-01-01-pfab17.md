


## Precomputation

Currently we iterate through each pixel in our source image and compute the ANSI color that is closest to it on-the-fly. But what if we already knew all the answers? What if we already knew the closest ANSI color for every possible pixel color? Then we wouldn't need to do any computation, and could instead instantly look up the closest ANSI color to each pixel color to it in almost zero seconds flat.

"Well yes," you might say, "if you know all the answers in advance then of course your algorithm will be faster. But there's no free lunch, and we still have to do all the work to find those answers."

"In fact," you might continue, "if we use this approach then the price of our lunch actually *increases*. If we use the on-the-fly method method for an 80x80 image then we have to calculate the closest ANSI color to a pixel color `80 * 80 = 6,400` times. But if we calculate the closest ANSI color to every possible pixel color in advance then we have to do `256 * 256 * 256 =~ 16,000,000` calculations, almost all of which will be wasted."

"Ooh, can we rewind a few minutes, I thought of a good zinger," you might say a few minutes later, "ah-hem: this is like saying that producing Lord of the Rings was easy because all the publisher had to do was press print and a best-selling book came out." Then if you were paying me for any of my blog posts, which you aren't, then you might ostentatiously demand a refund.

If you said all of this then you would be making reasonable points, although your tone would be unnecessarily beligerant. But you would be missing the subtleties.

What we count as a program's "speed" depends on the context. Programs are written to be used, and so arguably the most important definitions of "speed" is the time that a program takes to respond to a user's request. For our program, this is the time between a user asking for a colorful ASCII art image and our program printing it to the terminal. 

The precomputation approach may require more total work to be performed (although as we will see later, this is not always the case). However, we can do all of the precomputation in advance of the program being run. We can store the results in a file or a database, and load them when our program runs. This means that the execution time experienced by the user is minimal. There's no free lunch, but like any good investment bank we can palm the costs off onto someone else.

### Is this actually a good idea?

Whether this precomputation approach is actually a good idea depends on the specifics of how Justin intends his program to be used. Currently it is designed to ASCII-ize a single image at a time. This means that precomputation is unlikely to be a good idea. I generated a JSON file containing all the precomputated mappings from RGB values to the closest ANSI colors. It was a good 350MB in size. Even if we set aside the question of whether we want to devote that much space on our hard drive to this color map, a 350MB file still takes a long time (20 seconds or more) for our program to read off of our hard drive and into memory. For a single 80x80 image, this means that even though the image itself takes 90% less time to generate, the overall program takes 2000% more time to run from start to finish.

However, we only have to load our precomputed color map once per execution of our program. If we passed in a long list of images, instead of just a single one, loading our giant precomputed color map starts to look like a prudent idea. The overall running time of our program can be roughly calculated using the following equation:

```
total_time = startup_time + (time_per_image * n_images)
```

When `n_images` is small, `total_time` is dominated by the `startup_time` term. However, as `n_images` gets large, the `time_per_image * n_images` term starts to become more important. It becomes more important to minimize `time_per_image` than it is to minimize `startup_time`. In other words, the more images we process, the more sensible it becomes to accept a large `startup_time` if it reduces `time_per_image`. This is exactly what using our precomputed data achieves.

Using the precomputed approach also becomes more sensible the larger each image is. The precomputed approach saves us time on every pixel color we calculate. This intuitively means that the more pixels we are calculating, the more important it becomes to minimize time per pixel. We can rewrite the above equation in terms of pixels as follows:

```
total_time = startup_time + (time_per_pixel * pixels_per_image * n_images)
```

The larger `pixels_per_image` is, the larger the second term, and the more important it becomes to minimize `time_per_pixel` instead of `startup_time`.

### Let's get the best of both worlds

The fastest approach depends on the inputs our program receives. This means that if we really, *really* want to optimize our program's performance (which after all is the whole point of this post) then we should change our algorithm based on the inputs we receive. We could run our program repeatedly and profile how the different algorithms perform for different inputs. We could use this data to estimate how long each approach will take for a given new input. For example, if we are given 1000 images that are each 640x480 pixels in size, we might estimate that we should accept the startup performance hit of loading the precomputed color map. We have so many calculations to do that we will be able to efficiently *amortize* this cost over them all. By contrast, if we are given 2 images of 80x80 pixels in size, we might estimate that the on-the-fly approach will be quicker. All of this calculation can be done transparently in the background, without the user having to know or care what we are doing.

## Conclusion

At first glance, precomputing the closest ANSI color to every single RGB value sounds like a stupid, gluttonous idea. And in some situations it is.





 We can perform this calculation







We could give the user the option for which algorithm they want to use. We could even estimate how long each approach will take, and choose the fastest one.


Do you want people to have to download a giant database when they install your program?

This approach sounds stupid, but is actually kind of smart in some circumstances





Instead of finding the closest destination color to each source color on our image on-the-fly while we are analyzing the image, we can precompute the closest color for every possible source color before we have even loaded the image. We store all of this data in a map (or dictionary or hash or whatever your language calls it). Then, when we analyze the image, we don't have to do any calculations. Instead we just look up the current source color in our map, immediately finding the appropriate destination color.

Since we do all our calculations ahead of time, we don't have to do any heavy lifting while we are analyzing the image. This means that the time between loading the image and outputting a colorful ASCII-art version of it is shortened.

"But -" I hear you say "- we've just swept all the hard work under the rug! We still have to calculate the closest color `256 * 256 * 256 = 16.7MM` times. Surely we can't just pretend that all that work never happened? That would be like saying that producing Lord Of The Rings was easy because all the publisher has to do is press print and they've got themselves a book."

This is a good point, and serves as a good reminder that there is no such thing as a free lunch. However, it doesn't mean that we can't structure our lunch more efficiently so as to suit our particular requirements. It would be foolish to run this precomputation every time we run our program. However, we can instead run it once, then save the results in a file or a database for future reuse. We pay a very large up-front cost, but get to reuse the fruits of this work to speed up every colorful ASCII-art that we create.

This tradeoff won't always be worth it. If you only want to produce a single ASCII art picture then there's no point doing all this up-front work. But if you have an entire Dropbox account of Christmas pictures that you want to display on your terminal then you get to *amortize* the cost over a larger number of iterations.

## Caching

Sometimes full pre-computation isn't feasible. Perhaps the space of possible inputs is too large. In this situation you may want to instead use caching.

Caching is somewhat similar to precomputation. However, instead of precomputing every possible value before analyzing your image or images, you start analyzing immediately. But every time you find the closest destination color for a source color, you make a note of that mapping in your cache for future reference. The first time you try to map a source color, you have to do the calculation in full. But the second and subsequent times you can reuse the value you calculated the first time.

This has the advantage over full precompuation that you don't need to do any up-front work. The disadvantage is that it takes longer to process your image, especially if it doesn't feature many repeated colors. This disadvantage can be mitigated if you save and reuse your cache between images. As your cache gets fuller and fuller it starts to resemble the fully precomputed store.

How important these factors are to you depends on your specific application and your requirements. If you're just tinkering around on your own computer and just want to speed up your overall execution time a little then caching might be the most prudent approach. The program feels and runs very similarly to the non-optimized version, but doesn't do the same calculation twice.

However, if you're providing an online colorful ASCII-art-generation service then consistent speed becomes more important. You don't want any of your users to have to wait for their images, even those who make the first requests before your cache is highly populated. You might be more inclined to use pre-computation.

## Other examples of caching

### Photomosaics

In my *Programming Projects for Advanced Beginners #4: Photomosaics*[LINK], the reader takes a source image and converts it into a photomosaic, built using a large dump of mosaic iamges. This is a very similar challenge to producing colorful ASCII art. Instead of mapping sections of the source image to the most similar terminal colors, we need to map the sections to the most similar mosaic image.

This is slower than colorful ASCII art. For each section of the source image we have to:

1. Calculate the average color of the source image
2. Calculate the average color of each mosaic image
3. Find the mosaic image that is closest to the source image

Without any optimizations, the photomosaic program takes several minutes or more to run. This has an appreciable impact on the quality of life of the user, which means that it is acceptable and indeed sensible to try to speed the program up.

We can speed up the program using the same techniques we've discussed for speeding up the ASCII art program. We can even apply the techniques at two different levels.

First, we can reduce duplication of work in step 2 above - "calculate the average color of each mosaic image". Instead of re-doing these calculations for every source image, we can do them once and reuse the same results for each section of the source image. We're going to have to calculate the average color of every mosaic image for the very first source image section, so there is little practical difference between pre-computation and a cache.

Second, we can cache our results in step 3, in exactly the same way as for our colorful ASCII art. We can precompute the closest images to each RGB value, or compute the closest images on the fly and re-use our results when we next see the same source image color. If we want to avoid re-using the same mosaic image in our output image then we can return the top few closest matches and have the mosaic-building code randomly choose between them.

Once again, which approach is best depends on what we're trying to achieve.

### Twitter

I haven't been keeping up with exactly how Twitter generate user timelines, but I know that there's a lot of optimization involved. The naive version might look something like "select all tweets by all users that the current user follows, then order them by time created." This works for small numbers of users, but once you start to have a large number of users creating a large number of Tweets, it starts to become infeasible to run such finicky queries regularly.

Instead, Twitter generates and stores user timelines in advance. Whenever a user Tweets, a copy of the Tweet is inserted into the timeline of every user who follows that user. This effectively precomputes timelines; now when a user loads their timeline, Twitter doesn't have to scrabble around to collect all the Tweets by users that they follow and interleave them in the right order. Instead they can simply load all Tweets in the user's precomputed timeline, no extra work required.

I can't find the articles in which I read this, but I promise that they did exist. Even if they don't and I'm totally wrong, this would be a reasonable architecture for Twitter to use.

## Real-world complications

Our ASCII-art project is a particularly amenable to caching, because the calculation we are doing is a *pure function*. This means that it depends only on its inputs, and doesn't have to load any other data from an external source like a file or database. If the source color `(31, 42, 49)` maps to the destination color `(30, 40, 50)` when the function is first invoked, it will always map to that color from now until the end of time. A task like "give me all the comments on this post" requires more care and consideration of tradeoffs. How important is it to always give the exact, up-to-date, guaranteed-correct answer? Is this more important than saving a few milliseconds here and there? These tradeoffs will be different when considering social network posts and bank balances.

We are also lucky that the amount of data that we want to store in our cache is not that large. Suppose that we precompute every mapping from every source color to a destination color. Let's say that storing each mapping takes about 10 bytes. So storing the entire cache will only require `256 * 256 * 256 * 10 = 167MM bytes = 167MB`. This is not an insignificant amount of data, but it's easy for a modern computer to store the entire cache in memory.

Suppose instead that we were storing much more data in our cache. We only have a limited amount of memory that we can store our cache in, so eventually we'll have more data than we can fit in our cache. At this point we'll have to start making decisions about which data is most important to keep in our cache. There are many different strategies we can use to decide which data to *evict*; some commons ones are to evict the *least recently used* data first, or the *least frequently used*.




the speed of every image is important, even those request




Reader sent me code
Asked how to make it faster

First response to this should always be "why do you want to do that?"
His program runs in 0.001s, don't need to shave off 0.0001s
Make code faster if its speed is causing you problems
cf Photomosaic project - takes ages, makes iterating difficult
TODO: where else would this be useful in real life?

Faster code is often more complex
"Premature optimization..."
I believe that Knuth was talking about ridiculous optimizations that make your code harder to read
If you can make your code faster without making it gnarly, then OK fine

Any code that runs in the background doesn't need to be particularly fast
Might as well be as fast as you can
But I'd rather spend time and energy making it reliable, easy to read, etc

The general problem is "given a point and a set of other points, which other point is the closest?"
Then repeat this over and over again

The correct approach depends on your situation and other constraints
Not just in the sense of is this worth spending time on - in the sense of what is the best end state?

There's probably a fancy fast algorithm that speeds up the calculation
This might be the kind of optimization that Knuth was cautioning against
Could approximate by just finding L1 distance
Not generally applicable, not what I want to talk about

Generally applicable processes are pre-computation and caching
We're going to assume that we are dealing with integers. You'll see why

## Pre-computation

For every 256**3 combinations we calculate the closest ANSI color
Make a big dict
Then when we want to find closest color we just look it up in our dict
Dicts are O(1) to access, so this is very quick

But isn't this mad, we did 256**3 computations at the start??
Yes, but we only did them once. We can save their outputs and then reuse the results in every photo we do in the future
Could even do calculations to 0.1, 0.01 accuracy if we wanted

The limiting factor probably becomes loading the cached results
If we had 100000000000 elements then loading them all into memory at the start of the program would take ages
This still might be worth it! If we were processing loads of photos at once we could *amortize* this cost over them all
Unlikely to be worth it for a single photo

Maybe we would be better off loading the data into a DB
Maybe then accessing the DB takes longer than just doing the calculation!
This is more worthwhile the more expensive the original calculation is

## Caching

Don't pre-compute everything, but every time you do a calculation cache it so you can reuse the result if you need to
Useful if there are lots of repetition and you can't precompute everything for some reason. Maybe the space is too large
Could save your cache between images if you wanted

## So what does the code look like?

(Maybe next week? This could get gnarly)

Write to the same interface so it's easy to swap in and out
Probably use a class

```python
# Don't technically need to mention colors. How can
# you make this as generic and general as possible
# without doing anything stupid?
class ClosestPointFinder(object)

    def __init__(self, points):
        self.points = points

    def find(self, source_point):
        min_point = # ETC
```

```python
class MemoryCachedClosestPointFinder(object):

    def __init__(self, finder):
        self.finder = finder

# TODO: we can even abstract out the cache and the finder
```

Generic function cacher!