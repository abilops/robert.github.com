


## Precomputation

Currently we iterate through each pixel in our source image and compute the ANSI color that is closest to it on-the-fly. But what if we already knew the closest ANSI color for every possible pixel color? Then we wouldn't need to do any computation, and could instead take each pixel color and instantly look up the closest ANSI color to it in a fraction of the time.

"Well yes," you might say, "if you know all the answers in advance then of course your algorithm will be faster. But there's no free lunch, and we still have to do all the work to find those answers."

"In fact," you might continue, "using this approach the price of our lunch actually *increases*. Using the normal method for an 80x80 image we have to calculate the closest ANSI color to a pixel color `80 * 80 = 6,400` times. But if we calculate the closest ANSI color to every possible pixel color in advance then we have to do `256 * 256 * 256 =~ 16,000,000` calculations, almost all of which will be wasted."

"Ooh, can we rewind a few minutes, I thought of a good analogy," you might say a few minutes later, "ah-hem: this is like saying that producing Lord of the Rings was easy because all the publisher had to do was press print and a best-selling book came out." If you were paying me for any of these blog posts then you might ostentatiously demand a refund.

You would be making reasonable points, although your tone would be unnecessarily beligerant. But you would be missing the subtleties.

What we count as a program's "speed" depends on the context. Programs are written to be used, and so one of the most important definitions of "speed" is the time that a program takes to respond to a user's request. In our case, this is how long the time between a user asking for a colorful ASCII art image and our program printing it to the terminal. 





Instead of finding the closest destination color to each source color on our image on-the-fly while we are analyzing the image, we can precompute the closest color for every possible source color before we have even loaded the image. We store all of this data in a map (or dictionary or hash or whatever your language calls it). Then, when we analyze the image, we don't have to do any calculations. Instead we just look up the current source color in our map, immediately finding the appropriate destination color.

Since we do all our calculations ahead of time, we don't have to do any heavy lifting while we are analyzing the image. This means that the time between loading the image and outputting a colorful ASCII-art version of it is shortened.

"But -" I hear you say "- we've just swept all the hard work under the rug! We still have to calculate the closest color `256 * 256 * 256 = 16.7MM` times. Surely we can't just pretend that all that work never happened? That would be like saying that producing Lord Of The Rings was easy because all the publisher has to do is press print and they've got themselves a book."

This is a good point, and serves as a good reminder that there is no such thing as a free lunch. However, it doesn't mean that we can't structure our lunch more efficiently so as to suit our particular requirements. It would be foolish to run this precomputation every time we run our program. However, we can instead run it once, then save the results in a file or a database for future reuse. We pay a very large up-front cost, but get to reuse the fruits of this work to speed up every colorful ASCII-art that we create.

This tradeoff won't always be worth it. If you only want to produce a single ASCII art picture then there's no point doing all this up-front work. But if you have an entire Dropbox account of Christmas pictures that you want to display on your terminal then you get to *amortize* the cost over a larger number of iterations.

## Caching

Sometimes full pre-computation isn't feasible. Perhaps the space of possible inputs is too large. In this situation you may want to instead use caching.

Caching is somewhat similar to precomputation. However, instead of precomputing every possible value before analyzing your image or images, you start analyzing immediately. But every time you find the closest destination color for a source color, you make a note of that mapping in your cache for future reference. The first time you try to map a source color, you have to do the calculation in full. But the second and subsequent times you can reuse the value you calculated the first time.

This has the advantage over full precompuation that you don't need to do any up-front work. The disadvantage is that it takes longer to process your image, especially if it doesn't feature many repeated colors. This disadvantage can be mitigated if you save and reuse your cache between images. As your cache gets fuller and fuller it starts to resemble the fully precomputed store.

How important these factors are to you depends on your specific application and your requirements. If you're just tinkering around on your own computer and just want to speed up your overall execution time a little then caching might be the most prudent approach. The program feels and runs very similarly to the non-optimized version, but doesn't do the same calculation twice.

However, if you're providing an online colorful ASCII-art-generation service then consistent speed becomes more important. You don't want any of your users to have to wait for their images, even those who make the first requests before your cache is highly populated. You might be more inclined to use pre-computation.

## Other examples of caching

### Photomosaics

In my *Programming Projects for Advanced Beginners #4: Photomosaics*[LINK], the reader takes a source image and converts it into a photomosaic, built using a large dump of mosaic iamges. This is a very similar challenge to producing colorful ASCII art. Instead of mapping sections of the source image to the most similar terminal colors, we need to map the sections to the most similar mosaic image.

This is slower than colorful ASCII art. For each section of the source image we have to:

1. Calculate the average color of the source image
2. Calculate the average color of each mosaic image
3. Find the mosaic image that is closest to the source image

Without any optimizations, the photomosaic program takes several minutes or more to run. This has an appreciable impact on the quality of life of the user, which means that it is acceptable and indeed sensible to try to speed the program up.

We can speed up the program using the same techniques we've discussed for speeding up the ASCII art program. We can even apply the techniques at two different levels.

First, we can reduce duplication of work in step 2 above - "calculate the average color of each mosaic image". Instead of re-doing these calculations for every source image, we can do them once and reuse the same results for each section of the source image. We're going to have to calculate the average color of every mosaic image for the very first source image section, so there is little practical difference between pre-computation and a cache.

Second, we can cache our results in step 3, in exactly the same way as for our colorful ASCII art. We can precompute the closest images to each RGB value, or compute the closest images on the fly and re-use our results when we next see the same source image color. If we want to avoid re-using the same mosaic image in our output image then we can return the top few closest matches and have the mosaic-building code randomly choose between them.

Once again, which approach is best depends on what we're trying to achieve.

### Twitter

I haven't been keeping up with exactly how Twitter generate user timelines, but I know that there's a lot of optimization involved. The naive version might look something like "select all tweets by all users that the current user follows, then order them by time created." This works for small numbers of users, but once you start to have a large number of users creating a large number of Tweets, it starts to become infeasible to run such finicky queries regularly.

Instead, Twitter generates and stores user timelines in advance. Whenever a user Tweets, a copy of the Tweet is inserted into the timeline of every user who follows that user. This effectively precomputes timelines; now when a user loads their timeline, Twitter doesn't have to scrabble around to collect all the Tweets by users that they follow and interleave them in the right order. Instead they can simply load all Tweets in the user's precomputed timeline, no extra work required.

I can't find the articles in which I read this, but I promise that they did exist. Even if they don't and I'm totally wrong, this would be a reasonable architecture for Twitter to use.

## Real-world complications

Our ASCII-art project is a particularly amenable to caching, because the calculation we are doing is a *pure function*. This means that it depends only on its inputs, and doesn't have to load any other data from an external source like a file or database. If the source color `(31, 42, 49)` maps to the destination color `(30, 40, 50)` when the function is first invoked, it will always map to that color from now until the end of time. A task like "give me all the comments on this post" requires more care and consideration of tradeoffs. How important is it to always give the exact, up-to-date, guaranteed-correct answer? Is this more important than saving a few milliseconds here and there? These tradeoffs will be different when considering social network posts and bank balances.

We are also lucky that the amount of data that we want to store in our cache is not that large. Suppose that we precompute every mapping from every source color to a destination color. Let's say that storing each mapping takes about 10 bytes. So storing the entire cache will only require `256 * 256 * 256 * 10 = 167MM bytes = 167MB`. This is not an insignificant amount of data, but it's easy for a modern computer to store the entire cache in memory.

Suppose instead that we were storing much more data in our cache. We only have a limited amount of memory that we can store our cache in, so eventually we'll have more data than we can fit in our cache. At this point we'll have to start making decisions about which data is most important to keep in our cache. There are many different strategies we can use to decide which data to *evict*; some commons ones are to evict the *least recently used* data first, or the *least frequently used*.




the speed of every image is important, even those request




Reader sent me code
Asked how to make it faster

First response to this should always be "why do you want to do that?"
His program runs in 0.001s, don't need to shave off 0.0001s
Make code faster if its speed is causing you problems
cf Photomosaic project - takes ages, makes iterating difficult
TODO: where else would this be useful in real life?

Faster code is often more complex
"Premature optimization..."
I believe that Knuth was talking about ridiculous optimizations that make your code harder to read
If you can make your code faster without making it gnarly, then OK fine

Any code that runs in the background doesn't need to be particularly fast
Might as well be as fast as you can
But I'd rather spend time and energy making it reliable, easy to read, etc

The general problem is "given a point and a set of other points, which other point is the closest?"
Then repeat this over and over again

The correct approach depends on your situation and other constraints
Not just in the sense of is this worth spending time on - in the sense of what is the best end state?

There's probably a fancy fast algorithm that speeds up the calculation
This might be the kind of optimization that Knuth was cautioning against
Could approximate by just finding L1 distance
Not generally applicable, not what I want to talk about

Generally applicable processes are pre-computation and caching
We're going to assume that we are dealing with integers. You'll see why

## Pre-computation

For every 256**3 combinations we calculate the closest ANSI color
Make a big dict
Then when we want to find closest color we just look it up in our dict
Dicts are O(1) to access, so this is very quick

But isn't this mad, we did 256**3 computations at the start??
Yes, but we only did them once. We can save their outputs and then reuse the results in every photo we do in the future
Could even do calculations to 0.1, 0.01 accuracy if we wanted

The limiting factor probably becomes loading the cached results
If we had 100000000000 elements then loading them all into memory at the start of the program would take ages
This still might be worth it! If we were processing loads of photos at once we could *amortize* this cost over them all
Unlikely to be worth it for a single photo

Maybe we would be better off loading the data into a DB
Maybe then accessing the DB takes longer than just doing the calculation!
This is more worthwhile the more expensive the original calculation is

## Caching

Don't pre-compute everything, but every time you do a calculation cache it so you can reuse the result if you need to
Useful if there are lots of repetition and you can't precompute everything for some reason. Maybe the space is too large
Could save your cache between images if you wanted

## So what does the code look like?

(Maybe next week? This could get gnarly)

Write to the same interface so it's easy to swap in and out
Probably use a class

```python
# Don't technically need to mention colors. How can
# you make this as generic and general as possible
# without doing anything stupid?
class ClosestPointFinder(object)

    def __init__(self, points):
        self.points = points

    def find(self, source_point):
        min_point = # ETC
```

```python
class MemoryCachedClosestPointFinder(object):

    def __init__(self, finder):
        self.finder = finder

# TODO: we can even abstract out the cache and the finder
```

Generic function cacher!